{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "import plotly.graph_objects as go\n",
    "import pylab\n",
    "import warnings\n",
    "from scipy.stats import sem\n",
    "from datetime import datetime\n",
    "from termcolor import colored\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold, KFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pflXMP7hQN80"
   },
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJNO-gYMQN80"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('Akkumulated_tennis_PR.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NVvcq12QN80",
    "scrolled": false
   },
   "source": [
    "df=df.drop(['Unnamed: 0.1',\n",
    " 'Unnamed: 0',\n",
    " 'P1Ace',\n",
    " 'P2Ace',\n",
    " 'P1Winner',\n",
    " 'P2Winner',\n",
    " 'P1DoubleFault',\n",
    " 'P2DoubleFault',\n",
    " 'P1UnfErr',\n",
    " 'P2UnfErr',\n",
    " 'P1NetPoint',\n",
    " 'P2NetPoint',\n",
    " 'P1NetPointWon',\n",
    " 'P2NetPointWon',\n",
    " 'P1BreakPoint',\n",
    " 'P2BreakPoint',\n",
    " 'P1BreakPointWon',\n",
    " 'P2BreakPointWon',\n",
    " 'P1BreakPointMissed',\n",
    " 'P2BreakPointMissed',\n",
    " 'ServeIndicator',\n",
    " 'ServeNumber',\n",
    " 'WinnerType',\n",
    " 'WinnerShotType',\n",
    " 'P1DistanceRun',\n",
    " 'P2DistanceRun',\n",
    " 'RallyCount','P1_ServeWidth_0',\n",
    " 'P1_ServeWidth_B',\n",
    " 'P1_ServeWidth_BC',\n",
    " 'P1_ServeWidth_BW',\n",
    " 'P1_ServeWidth_C',\n",
    " 'P1_ServeWidth_DoubleFault',\n",
    " 'P1_ServeWidth_W',\n",
    " 'P1_ServeDepth_0',\n",
    " 'P1_ServeDepth_CTL',\n",
    " 'P1_ServeDepth_DoubleFault',\n",
    " 'P1_ServeDepth_NCTL',\n",
    " 'P1_ReturnDepth_0',\n",
    " 'P1_ReturnDepth_D',\n",
    " 'P1_ReturnDepth_DoubleFault',\n",
    " 'P1_ReturnDepth_ND',\n",
    " 'P1_ReturnDepth_ServeAce',\n",
    " 'P1_ReturnDepth_Service box',\n",
    " 'P2_ServeWidth_0',\n",
    " 'P2_ServeWidth_B',\n",
    " 'P2_ServeWidth_BC',\n",
    " 'P2_ServeWidth_BW',\n",
    " 'P2_ServeWidth_C',\n",
    " 'P2_ServeWidth_DoubleFault',\n",
    " 'P2_ServeWidth_W',\n",
    " 'P2_ServeDepth_0',\n",
    " 'P2_ServeDepth_CTL',\n",
    " 'P2_ServeDepth_DoubleFault',\n",
    " 'P2_ServeDepth_NCTL',\n",
    " 'P2_ReturnDepth_0',\n",
    " 'P2_ReturnDepth_D',\n",
    " 'P2_ReturnDepth_DoubleFault',\n",
    " 'P2_ReturnDepth_ND',\n",
    " 'P2_ReturnDepth_ServeAce',\n",
    " 'P2_ReturnDepth_Service box',\n",
    " 'ElapsedTime',\n",
    " 'ServeWidth',\n",
    " 'ServeDepth',\n",
    " 'P1Momentum',\n",
    " 'P2Momentum',\n",
    " 'ReturnDepth',\n",
    " 'datetime',\n",
    " 'year',\n",
    " 'slam',\n",
    " 'match_num',\n",
    " 'player1',\n",
    " 'player2',\n",
    " 'tourney_name',\n",
    " 'tourney_date',\n",
    " 'winner_name',\n",
    " 'winner_hand',\n",
    " 'loser_name',\n",
    " 'loser_hand'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making rank for p1 and p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['match_id'] == \"2020-usopen-1228\", 'loser_rank'] = 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_rank = []\n",
    "p2_rank = []\n",
    "for i in range(len(df)):\n",
    "    if df.player1.iloc[i] == df.winner_name.iloc[i]:\n",
    "        p1_rank.append(df.winner_rank.iloc[i])\n",
    "        p2_rank.append(df.loser_rank.iloc[i])\n",
    "    if df.player1.iloc[i] == df.loser_name.iloc[i]:\n",
    "        p1_rank.append(df.loser_rank.iloc[i])\n",
    "        p2_rank.append(df.winner_rank.iloc[i])\n",
    "df['P1Rank'] = p1_rank\n",
    "df['P2Rank'] = p2_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making total elapsed time in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time=[]\n",
    "for i in range(len(df.match_id.unique())):\n",
    "    total_time = np.append(total_time, np.insert(df[df['match_id'] == df.match_id.unique()[i]].Point_lenght_sec.cumsum().values[:-1],0,0))\n",
    "df['Total_time']=total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a tiebreak variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to be shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_shifted=[\n",
    "    ('P1GamesWon','P2GamesWon'),\n",
    "    ('P1Score','P2Score'),\n",
    "    ('P1PointsWon','P2PointsWon'),\n",
    "    ('P1_ServeWidth_B_A','P2_ServeWidth_B_A'),\n",
    "    ('P1_ServeWidth_BC_A','P2_ServeWidth_BC_A'),\n",
    "    ('P1_ServeWidth_BW_A','P2_ServeWidth_BW_A'),\n",
    "    ('P1_ServeWidth_C_A','P2_ServeWidth_C_A'),\n",
    "    ('P1_ServeWidth_W_A','P2_ServeWidth_W_A'),\n",
    "    ('P1_ServeDepth_CTL_A','P2_ServeDepth_CTL_A'),\n",
    "    ('P1_ServeDepth_NCTL_A','P2_ServeDepth_NCTL_A'),\n",
    "    ('P1_ReturnDepth_D_A','P2_ReturnDepth_D_A'),\n",
    "    ('P1_ReturnDepth_ND_A','P2_ReturnDepth_ND_A'),\n",
    "    ('P1AceA','P2AceA'),\n",
    "    ('P1WinnerA','P2WinnerA'),\n",
    "    ('P1DoubleFaultA','P2DoubleFaultA'),\n",
    "    ('P1UnfErrA','P2UnfErrA'),\n",
    "    ('P1NetPointA','P2NetPointA'),\n",
    "    ('P1NetPointWonA','P2NetPointWonA'),\n",
    "    ('P1BreakPointA','P2BreakPointA'),\n",
    "    ('P1BreakPointWonA','P2BreakPointWonA'),\n",
    "    ('P1BreakPointMissedA','P2BreakPointMissedA'),\n",
    "    ('P1DistanceRunA','P1DistanceRunA'),\n",
    "    ('P1SetsWon','P2SetsWon'),\n",
    "    ('player1','player2'),\n",
    "    ('P1NetPoint','P2NetPoint'),\n",
    "    ('P1Rank','P2Rank')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing pointwinenr to binary encoding 0 = p1, 1= p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Server = {1: 0,2: 1}\n",
    "df.PointServer = [Server[item] for item in df.PointServer]\n",
    "df.PointWinner = [Server[item] for item in df.PointWinner]\n",
    "df.GameWinnerA = [Server[item] for item in df.GameWinnerA]\n",
    "df.SetWinnerA = [Server[item] for item in df.SetWinnerA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surface attribute, 0 = US open (Turf), 1 = Wimbeldon grass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface=[]\n",
    "for i in range(len(df)):\n",
    "    if df.match_id.iloc[i][5] == \"u\":\n",
    "        surface.append(0)\n",
    "    else:\n",
    "        surface.append(1)\n",
    "df['Surface']=surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TieBreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tie=[]\n",
    "for i in range(len(df)):\n",
    "    if (df.GameNo.iloc[i]>12) and (df.Surface.iloc[i]==0):\n",
    "        tie.append(1)\n",
    "    elif (df.GameNo.iloc[i] == 25) and (df.Surface.iloc[i]==1) and (df.year.iloc[i]>2018):\n",
    "        tie.append(1)\n",
    "    elif (df.GameNo.iloc[i] > 12) and (df.Surface.iloc[i]==1) and (df.SetNo.iloc[i] < 5):\n",
    "        tie.append(1)\n",
    "    else:\n",
    "        tie.append(0)\n",
    "df['Tiebreak']=tie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing point number mistakes in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V6llyDtQN81"
   },
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    if pd.isna(df.iloc[i]['PointNumber']):\n",
    "        df['PointNumber'].iloc[i]=df['PointNumber'].iloc[i+1]-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing NA value for loser rank in a single match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making unique time stamps for what point number in the game it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_timestamp=[]\n",
    "\n",
    "k=0\n",
    "for i in range(len(df)):\n",
    "    if df.GameNo.iloc[i] != df.GameNo.iloc[i-1]:\n",
    "        k=1\n",
    "        game_timestamp.append(k)\n",
    "    if df.GameNo.iloc[i] == df.GameNo.iloc[i-1]:\n",
    "        k=k+1\n",
    "        game_timestamp.append(k)\n",
    "df['Game_timestamp']=game_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game_test_columns=['SetNo',\n",
    "                   'P1GamesWon',\n",
    "                   'P2GamesWon',\n",
    "                   'GameNo',\n",
    "                   'PointNumber',\n",
    "                   #'PointServer',\n",
    "                   'Tiebreak',\n",
    "                   'P1Score',\n",
    "                   'P2Score',\n",
    "                   'P1PointsWon',\n",
    "                   'P2PointsWon',\n",
    "                   #'Point_lenght_sec',\n",
    "                   'P1_ServeWidth_B_A',\n",
    "                   'P1_ServeWidth_BC_A',\n",
    "                   'P1_ServeWidth_BW_A',\n",
    "                   'P1_ServeWidth_C_A',\n",
    "                   'P1_ServeWidth_W_A',\n",
    "                   'P1_ServeDepth_CTL_A',\n",
    "                   'P1_ServeDepth_NCTL_A',\n",
    "                   'P1_ReturnDepth_D_A',\n",
    "                   'P1_ReturnDepth_ND_A',\n",
    "                   'P2_ServeWidth_B_A',\n",
    "                   'P2_ServeWidth_BC_A',\n",
    "                   'P2_ServeWidth_BW_A',\n",
    "                   'P2_ServeWidth_C_A',\n",
    "                   'P2_ServeWidth_W_A',\n",
    "                   'P2_ServeDepth_CTL_A',\n",
    "                   'P2_ServeDepth_NCTL_A',\n",
    "                   'P2_ReturnDepth_D_A',\n",
    "                   'P2_ReturnDepth_ND_A',\n",
    "                   'P1AceA',\n",
    "                   'P2AceA',\n",
    "                   'P1WinnerA',\n",
    "                   'P2WinnerA',\n",
    "                   'P1DoubleFaultA',\n",
    "                   'P2DoubleFaultA',\n",
    "                   'P1UnfErrA',\n",
    "                   'P2UnfErrA',\n",
    "                   'P1NetPointA',\n",
    "                   'P2NetPointA',\n",
    "                   'P1NetPointWonA',\n",
    "                   'P2NetPointWonA',\n",
    "                   'P1BreakPointA',\n",
    "                   'P2BreakPointA',\n",
    "                   'P1BreakPointWonA',\n",
    "                   'P2BreakPointWonA',\n",
    "                   'P1BreakPointMissedA',\n",
    "                   'P2BreakPointMissedA',\n",
    "                   'P1DistanceRunA',\n",
    "                   'P2DistanceRunA',\n",
    "                   'RallyCountA',\n",
    "                   'P1SetsWon',\n",
    "                   'P2SetsWon',\n",
    "                   'Game_timestamp',\n",
    "                   'P1Rank',\n",
    "                   'P2Rank',\n",
    "                   'Total_time',\n",
    "                   'Surface']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=0\n",
    "for i in range(len(df)-1):\n",
    "    if df2['P2GamesWon'].loc[i] > df2.P2GamesWon.loc[i+1] and df2['P2GamesWon'].loc[i+1] != 0:\n",
    "        k=k+1\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2[57924:57928]  Der er en fejl i P2GamesWOn her"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()\n",
    "for i in range(len(df)):\n",
    "    if df['PointServer'].loc[i] == 1:\n",
    "        for j in range(len(to_be_shifted)):\n",
    "            df.at[i,to_be_shifted[j][0]] = df2[to_be_shifted[j][1]].loc[i]\n",
    "            df.at[i,to_be_shifted[j][1]] = df2[to_be_shifted[j][0]].loc[i]\n",
    "        if df.PointWinner.loc[i] == 0:\n",
    "            df.at[i,\"PointWinner\"] = 1\n",
    "        elif df.PointWinner.loc[i] == 1:\n",
    "            df.at[i,\"PointWinner\"] = 0\n",
    "            \n",
    "        if df.GameWinnerA.loc[i] == 0:\n",
    "            df.at[i,\"GameWinnerA\"]=1\n",
    "        elif df.GameWinnerA.loc[i] == 1: \n",
    "            df.at[i,\"GameWinnerA\"] = 0\n",
    "\n",
    "        if df.SetWinnerA.loc[i] == 0:\n",
    "            df.at[i,\"SetWinnerA\"]=1\n",
    "        elif df.SetWinnerA.loc[i] == 1: \n",
    "            df.at[i,\"SetWinnerA\"] = 0\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(Y_test,y_pred,x_val,model):\n",
    "    val_pred=y_pred\n",
    "    x_val=x_val\n",
    "    model=model\n",
    "    auc=round(accuracy_score(Y_test,val_pred),4)\n",
    "    f_score=round(f1_score(Y_test, val_pred),4)\n",
    "    P_score=round(precision_score(Y_test,val_pred),4)\n",
    "    recall=round(recall_score(Y_test, val_pred),4)\n",
    "    \n",
    "    ns_probs = [0 for _ in range(len(Y_test))]\n",
    "    lr_probs = model.predict_proba(x_val)\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(Y_test, ns_probs)\n",
    "    lr_auc = round(roc_auc_score(Y_test, lr_probs),4)\n",
    "    return [auc,f_score,P_score,recall,lr_auc]\n",
    "def averages1(name,metrics):\n",
    "    \n",
    "    res = [round(sum(x) / len(x),3) for x in zip(*metrics)]\n",
    "    print(f\"{name} Accuracy score: {res[0]*100}, f1 score: {res[1]}, Precision: {res[2]}, Recall: {res[3]}, ROC-AUC: {res[4]}\")\n",
    "    return\n",
    "\n",
    "def averages(metrics):\n",
    "    \n",
    "    res = [round(sum(x) / len(x),3) for x in zip(*metrics)]\n",
    "    #print(f\"{name} Accuracy score: {res[0]*100}, f1 score: {res[1]}, Precision: {res[2]}, Recall: {res[3]}, ROC-AUC: {res[4]}\")\n",
    "    return res\n",
    "\n",
    "def data_split(df):\n",
    "    matches=list(df.match_id.unique())\n",
    "    Val_games=random.sample(matches,round(len(matches)/10))\n",
    "    df_val=df[df['match_id'].isin(Val_games)]\n",
    "    df=df[~df['match_id'].isin(Val_games)]\n",
    "    return df,df_val\n",
    "\n",
    "\n",
    "def plot_feature_importance(importance,names,model_type):\n",
    "\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = importance\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    fi_df = fi_df[fi_df.feature_importance > 0.01]\n",
    "    \n",
    "\n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.tight_layout()\n",
    "    plt.yticks(fontsize=7)\n",
    "    #Plot Searborn bar chart\n",
    "    ax=sns.barplot(y='feature_names',x='feature_importance',data=fi_df,palette=\"rocket\")\n",
    "    ax.set_xlabel('feature_importance')\n",
    "    #plt.xticks(rotation = 90)\n",
    "    \n",
    "    #Add chart labels\n",
    "    plt.title(model_type + ' FEATURE IMPORTANCE')\n",
    "    plt.xlabel('Feature importance (Gain) > 0')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "\n",
    "\n",
    "def evaluation_time(y_val,y_pred,model,x_val):\n",
    "    plt.style.use('ggplot')\n",
    "    cm = confusion_matrix(y_val, (val_pred))\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax,fmt='g',cmap='rocket'); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted Winner');ax.set_ylabel('Actual Winner'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['Server', 'Returner']); ax.yaxis.set_ticklabels(['Server', 'Returner'], rotation=360);\n",
    "    plt.show()\n",
    "\n",
    "    f_score=f1_score(y_val, val_pred, average=None)\n",
    "    P_score=precision_score(y_val,val_pred,average=None)\n",
    "    recall=recall_score(y_val, val_pred, average=None)\n",
    "\n",
    "\n",
    "    ns_probs = [0 for _ in range(len(y_val))]\n",
    "    lr_probs = model.predict_proba(x_val)\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_val, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_val, lr_probs)\n",
    "    # summarize scores\n",
    "    print(f\"Test set accuracy score {(np.round(accuracy_score(y_pred,y_val),3)*100)}%\")\n",
    "    print(f\"F1 score {np.round(f_score,3)}\")\n",
    "    print(f\"Recall score {np.round(recall,3)}\")\n",
    "    print(f\"Precision score {np.round(P_score,3)}\")\n",
    "    print('Model: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_val, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_val, lr_probs)\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Base')\n",
    "    plt.plot(lr_fpr, lr_tpr, marker='.', label='XGBoost')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluation(y_val,y_pred,model_xg,x_val):\n",
    "    plt.style.use('ggplot')\n",
    "    cm = confusion_matrix(y_val, (val_pred))\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax,fmt='g',cmap='rocket'); #annot=True to annotate cells\n",
    "\n",
    "    ax.set_xlabel('Predcited Winner');ax.set_ylabel('Actual Winner'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['Server', 'Returner']); ax.yaxis.set_ticklabels(['Server', 'Returner'], rotation=360);\n",
    "    plt.show()\n",
    "\n",
    "    f_score=f1_score(y_val, val_pred, average=None)\n",
    "    P_score=precision_score(y_val,val_pred,average=None)\n",
    "    recall=recall_score(y_val, val_pred, average=None)\n",
    "\n",
    "\n",
    "    ns_probs = [0 for _ in range(len(y_val))]\n",
    "    lr_probs = model_xg.predict_proba(x_val)\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_val, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_val, lr_probs)\n",
    "    # summarize scores\n",
    "    print(f\"Test set accuracy score {np.round(val_scoreXG*100,3)}%\")\n",
    "    print(f\"F1 score {np.round(f_score,3)}\")\n",
    "    print(f\"Recall score {np.round(recall,3)}\")\n",
    "    print(f\"Precision score {np.round(P_score,3)}\")\n",
    "    print('Xgboost: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_val, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_val, lr_probs)\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Base')\n",
    "    plt.plot(lr_fpr, lr_tpr, marker='.', label='XGBoost')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    results = model_xg.evals_result()\n",
    "    epochs = len(results['validation_0']['error'])\n",
    "    x_axis = range(0, epochs)\n",
    "    # plot log loss\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "    ax.legend()\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.title('XGBoost Log Loss')\n",
    "    plt.show()\n",
    "    # plot classification error\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "    ax.legend()\n",
    "    plt.ylabel('Classification Error')\n",
    "    plt.title('XGBoost Classification Error')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def errors(values):\n",
    "    errors=[]\n",
    "    for i in range(len(values)):\n",
    "        errors.append(sem(values[i])*100)\n",
    "    return errors\n",
    "\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "        \n",
    "def sample(x,y):\n",
    "    xx=np.unique(y,return_counts=True)\n",
    "    xx=xx[1][0]/xx[1][1]\n",
    "    if xx < 4:\n",
    "        return x,y\n",
    "    if xx > 4:\n",
    "        oversample = SMOTE(0.5)\n",
    "        x, y = oversample.fit_resample(x, y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Train and test set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df.copy()\n",
    "df,df_val=data_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1mJjFuDQN82"
   },
   "source": [
    "# GameWinner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define timestamps and show data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamps = np.unique(df['Game_timestamp'])\n",
    "\n",
    "l=[]\n",
    "\n",
    "for i in range(len(df)-1):\n",
    "    if df.Game_timestamp.iloc[i] > df.Game_timestamp.iloc[i+1]:\n",
    "        l.append(df.Game_timestamp.iloc[i])\n",
    "\n",
    "from collections import Counter\n",
    "c=Counter(l)\n",
    "plt.bar(c.keys(), c.values())\n",
    "plt.title('Number of points in games ')\n",
    "plt.xlabel('Number of points in game')\n",
    "plt.ylabel('Number of games')\n",
    "plt.show()\n",
    "\n",
    "l2=[]\n",
    "for i in range(1,max(df.Game_timestamp)):\n",
    "    l2.append(len(df[df.Game_timestamp==i]))\n",
    "print(l2)\n",
    "plt.plot(l2)\n",
    "plt.title('Number of rows for each time stamp (Points/Games) ')\n",
    "plt.xlabel('Points in game')\n",
    "plt.ylabel('Number of rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial GameWinner Model - UNTUNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for timestamps\n",
    "\n",
    "df,df_val=data_split(df_copy)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "RFC_features=[]\n",
    "XGB_features=[]\n",
    "LR_features=[]\n",
    "val=[]\n",
    "ADA_features=[]\n",
    "Model_performances=[]\n",
    "FI_XGB_final=[]\n",
    "\n",
    "for i in range(0,time_stamps[19]):\n",
    "    \n",
    "    if l2[i] > 300:\n",
    "        y=df[(df.Game_timestamp==i+1) & (df.Tiebreak == 0) ]['GameWinnerA'].values\n",
    "        x=df[(df.Game_timestamp==i+1) & (df.Tiebreak == 0)][Game_test_columns].values\n",
    "        y_val=df_val[(df_val.Game_timestamp==i+1)&(df_val.Tiebreak == 0)]['GameWinnerA'].values\n",
    "        x_val=df_val[(df_val.Game_timestamp==i+1)&(df_val.Tiebreak == 0)][Game_test_columns].values\n",
    "        #x, y = sample(x,y)\n",
    "        labels=Game_test_columns\n",
    "        scaler = StandardScaler()\n",
    "        x=scaler.fit_transform(x)\n",
    "        x_val=scaler.transform(x_val)\n",
    "\n",
    "        folds = 10\n",
    "        kf = KFold(n_splits=folds, random_state=42, shuffle=True)\n",
    "        RFCScore = []\n",
    "        ADAScore = []\n",
    "        XGBScore = []\n",
    "        LRScore=[]\n",
    "        FI_rfc = []\n",
    "        FI_ada = []\n",
    "        FI_XGB = []\n",
    "        FI_LR = []\n",
    "        for train_index, test_index in kf.split(x):\n",
    "            m=[]\n",
    "\n",
    "            X_train, X_test = x[train_index], x[test_index]\n",
    "            Y_train, Y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "        #RF\n",
    "\n",
    "            RFC = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "            predRFC = RFC.fit(X_train,Y_train)\n",
    "            y_predRFC = predRFC.predict(X_test)\n",
    "            scoreRFC = accuracy_score(y_predRFC, Y_test)\n",
    "            RFCScore.append(scoreRFC)\n",
    "            FI_rfc.append(metrics(Y_test,y_predRFC,X_test,RFC))\n",
    "\n",
    "        #ADA\n",
    "\n",
    "            ada = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=2, random_state=0))\n",
    "            predADA = ada.fit(X_train, Y_train)\n",
    "            y_predADA = predADA.predict(X_test)\n",
    "            scoreADA = accuracy_score(y_predADA, Y_test)\n",
    "            ADAScore.append(scoreADA)\n",
    "            FI_ada.append(metrics(Y_test,y_predADA,X_test,ada))\n",
    "\n",
    "\n",
    "        #XG\n",
    "            model_xg = XGBClassifier(objective ='binary:logistic',)\n",
    "            model_xg.fit(X_train, Y_train);\n",
    "            y_pred_xg = model_xg.predict(X_test);\n",
    "            scoreXG = accuracy_score(y_pred_xg, Y_test)\n",
    "            XGBScore.append(scoreXG)\n",
    "            FI_XGB.append(metrics(Y_test,y_pred_xg,X_test,model_xg))\n",
    "            \n",
    "        \n",
    "        #LR\n",
    "            logreg = LogisticRegression()\n",
    "            logreg.fit(X_train, Y_train)\n",
    "            y_pred_lr=logreg.predict(X_test)\n",
    "            scoreLR=accuracy_score(y_pred_lr, Y_test)\n",
    "            LRScore.append(scoreLR)\n",
    "            FI_LR.append(metrics(Y_test,y_pred_lr,X_test,logreg))\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=42)\n",
    "        model_xg = XGBClassifier(objective ='binary:logistic',min_child_weight=5,gamma=0.2,colsample_bytree=0.9,\n",
    "                                     max_depth=6,learning_rate = 0.01,scale_pos_weight=1.3,verbosity=0,use_label_encoder=False)\n",
    "        model_xg.fit(X_train, y_train);\n",
    "        y_pred_xg = model_xg.predict(X_test);\n",
    "        scoreXG = accuracy_score(y_pred_xg, y_test)\n",
    "        #print(f\"Model training score {round(scoreXG,2)}\")\n",
    "        val_pred=model_xg.predict(x_val)\n",
    "        val_scoreXG = accuracy_score(val_pred, y_val)\n",
    "        #print(f\"Validation set score {round(val_scoreXG,3)}\")\n",
    "        val.append(val_scoreXG)\n",
    "        FI_XGB_final.append(model_xg.feature_importances_)\n",
    "        print(f\"Predicted dist: {np.unique(val_pred, return_counts=True)}\")\n",
    "        print(f\"Actual Dist: {np.unique(y_val,return_counts=True)}\")\n",
    "\n",
    "\n",
    "    #i+=1\n",
    "    #print(f\"Processing fold {i}\")\n",
    "        XGB_features.append(averages(FI_XGB))\n",
    "        RFC_features.append(averages(FI_rfc))\n",
    "        ADA_features.append(averages(FI_ada))\n",
    "        LR_features.append(averages(FI_LR))\n",
    "        #rf_scores.append(np.mean(RFCScore)*100)\n",
    "        #lr_scores.append(np.mean(LRScore)*100)\n",
    "        #xg_scores.append(np.mean(XGBScore)*100)\n",
    "        m.append(np.mean(RFCScore)*100)\n",
    "        m.append(np.mean(XGBScore)*100)\n",
    "        m.append(np.mean(LRScore)*100)\n",
    "        m.append(np.mean(ADAScore)*100)\n",
    "        \n",
    "        Model_performances.append(m)\n",
    "    \n",
    "        print(f\"\\nModel Accuracy for timestamp {i+1} \")\n",
    "        #print(f\"Random Forest: {np.round(np.mean(RFCScore)*100,2)}% ---- XGboost: {np.round(np.mean(XGBScore)*100,2)}% .. XG_Test: {np.round(val_scoreXG*100,2)}% ---- Logistic Regression: {np.round(np.mean(LRScore)*100,2)}% ----  \")\n",
    "        #---- Adaboost: {np.round(np.mean(ADAScore)*100,2)}\n",
    "        #evaluation_time(y_val,val_pred,model_xg,x_val)\n",
    "warnings.filterwarnings('default')\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot([item[3] for item in Model_performances],label=('Adaboost'),color=\"red\")\n",
    "#plt.plot(np.array(val)*100,label=('XG_test'))\n",
    "plt.plot([item[2] for item in Model_performances],label=('Logistic regression'),color=\"Purple\")\n",
    "plt.plot([item[1] for item in Model_performances],label=('XGboost'),color=\"yellow\")\n",
    "plt.plot([item[0] for item in Model_performances],label=('Random Forest'),color=\"blue\")\n",
    "plt.plot(np.full(len(Model_performances), 80.5),label=(\"BaseLine\"),linestyle=(\"dashed\"),color=\"black\")\n",
    "plt.title('Model Accuracy for each time stamp')\n",
    "plt.xticks(range(0,14),np.arange(1,14+1))\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlabel('Time Stamps (Point number in game)')\n",
    "plt.ylabel('Accuracy score [%]');\n",
    "#print(f\"\\nLogistic regression: {round(np.mean([item[2] for item in Model_performances]),2)}\\nXGboost {round(np.mean([item[1] for item in Model_performances]),2)}\\nRandom Forest {round(np.mean([item[0] for item in Model_performances]),2)}, \\nAdaboost: {round(np.mean([item[3] for item in Model_performances]),2)}\")\n",
    "averages1(\"XGB\",XGB_features)\n",
    "averages1(\"RFC\",RFC_features)\n",
    "averages1(\"LR\",LR_features)\n",
    "averages1(\"ADA\",ADA_features)\n",
    "print(f\"XG_Test {round(np.mean(np.array(val)*100),2)}\")\n",
    "plt.savefig(\"Gamewinner_time_untuned\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(FI_XGB_final)):\n",
    "    plot_feature_importance(FI_XGB_final[i],labels,\"XG\")\n",
    "    plt.title(f\"Timestamp {i+1}\")\n",
    "    plt.savefig(f\"Game_winner_feature{i+1}\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost_hyper_time=[]\n",
    "for i in range(len(Model_performances)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[df.Game_timestamp==i+1][Game_test_columns].values,df[df.Game_timestamp==i+1]['GameWinnerA'].values, test_size=0.20, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test) \n",
    "    classifier=XGBClassifier(objective='binary:logistic')\n",
    "    #grid_search=GridSearchCV(classifier,param_grid=params,scoring='accuracy',n_jobs=-1,cv=10)\n",
    "\n",
    "    #start_time = timer(None)\n",
    "    #grid_search.fit(X_train,y_train)\n",
    "    #timer(start_time) # timing ends here for \"start_time\" variable\n",
    "    \n",
    "    random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=50,scoring='accuracy',n_jobs=-1,cv=10,verbose=3)\n",
    "    start_time = timer(None)\n",
    "    random_search.fit(X_train,y_train)\n",
    "    timer(start_time) # timing ends here for \"start_time\" variable\n",
    "    \n",
    "    \n",
    "    xgboost_hyper_time.append(random_search.best_params_)\n",
    "with open('Gird_search_game_time', 'w') as fout:\n",
    "    json.dump(xgboost_hyper_time, fout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Gird_search_game_time') as json_file:\n",
    "    params_game = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df,df_val=data_split(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GameWinner Timestamp Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "RFC_features=[]\n",
    "XGB_features=[]\n",
    "LR_features=[]\n",
    "val=[]\n",
    "ADA_features=[]\n",
    "Model_performances=[]\n",
    "FI_XGB_final=[]\n",
    "error_xg=[]\n",
    "\n",
    "for i in range(0,time_stamps[19]):\n",
    "    \n",
    "    if l2[i] > 300:\n",
    "        y=df[(df.Game_timestamp==i+1) & (df.Tiebreak == 0) ]['GameWinnerA'].values\n",
    "        x=df[(df.Game_timestamp==i+1) & (df.Tiebreak == 0)][Game_test_columns].values\n",
    "        y_val=df_val[(df_val.Game_timestamp==i+1)&(df_val.Tiebreak == 0)]['GameWinnerA'].values\n",
    "        x_val=df_val[(df_val.Game_timestamp==i+1)&(df_val.Tiebreak == 0)][Game_test_columns].values\n",
    "        #x, y = sample(x,y)\n",
    "        labels=Game_test_columns\n",
    "        scaler = StandardScaler()\n",
    "        x=scaler.fit_transform(x)\n",
    "        x_val=scaler.transform(x_val)\n",
    "\n",
    "        folds = 10\n",
    "        kf = KFold(n_splits=folds, random_state=42, shuffle=True)\n",
    "        RFCScore = []\n",
    "        ADAScore = []\n",
    "        XGBScore = []\n",
    "        LRScore=[]\n",
    "        FI_rfc = []\n",
    "        FI_ada = []\n",
    "        FI_XGB = []\n",
    "        FI_LR = []\n",
    "        for train_index, test_index in kf.split(x):\n",
    "            m=[]\n",
    "            \n",
    "        error_fill=[]\n",
    "        for j in range(10):\n",
    "            df_xg,df_val_xg=data_split(df_copy)\n",
    "            y=df_xg[(df_xg.Game_timestamp==i+1)&(df_xg.Tiebreak == 0)]['GameWinnerA'].values\n",
    "            x=df_xg[(df_xg.Game_timestamp==i+1)&(df_xg.Tiebreak == 0)][Game_test_columns].values\n",
    "            y_val=df_val_xg[(df_val_xg.Game_timestamp==i+1)&(df_val_xg.Tiebreak == 0)]['GameWinnerA'].values\n",
    "            x_val=df_val_xg[(df_val_xg.Game_timestamp==i+1)&(df_val_xg.Tiebreak == 0)][Game_test_columns].values\n",
    "            #x, y = sample(x,y)\n",
    "            labels=Game_test_columns\n",
    "            x=scaler.transform(x)\n",
    "            x_val=scaler.transform(x_val)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=42)\n",
    "            #model_xg = XGBClassifier(objective ='binary:logistic',)\n",
    "            model_xg = XGBClassifier(objective ='binary:logistic',params=params_game[i])\n",
    "            model_xg.fit(X_train, y_train);\n",
    "            val_pred=model_xg.predict(x_val)\n",
    "            val_scoreXG = accuracy_score(val_pred, y_val)\n",
    "            #print(f\"Validation set score {round(val_scoreXG,3)}\")\n",
    "            error_fill.append(val_scoreXG)\n",
    "            \n",
    "        error_xg.append(error_fill)    \n",
    "        val.append(val_scoreXG)\n",
    "        FI_XGB_final.append(model_xg.feature_importances_)\n",
    "        print(f\"Predicted dist: {np.unique(val_pred, return_counts=True)}\")\n",
    "        print(f\"Actual Dist: {np.unique(y_val,return_counts=True)}\")\n",
    "\n",
    "        evaluation_time(y_val,val_pred,model_xg,x_val)\n",
    "warnings.filterwarnings('default')\n",
    "plt.figure(figsize=(15,8))\n",
    "#plt.plot([item[3] for item in Model_performances],label=('Adaboost'))\n",
    "error=errors(error_xg)\n",
    "plt.plot(np.array(val)*100,label=('XG_test'))\n",
    "plt.fill_between(np.arange(0,len(error)), np.array(val)*100-error, np.array(val)*100+error,alpha=0.5)\n",
    "plt.plot(np.full(len(Model_performances), 80.5),label=(\"BaseLine\"),linestyle=(\"dashed\"),color=\"black\")\n",
    "plt.title('Model Accuracy for each Game time_stamp')\n",
    "plt.xticks(range(len(error)),np.arange(1,len(error)+1,1))\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlabel('Time Stamps (Point number in game)')\n",
    "plt.ylabel('Accuracy score [%]');\n",
    "print(f\"XG_Test {round(np.mean(np.array(val)*100),2)}\")\n",
    "#\\n---Average Accuracy--- \\nAdaboost: {round(np.mean([item[3] for item in Model_performances]),2)}\n",
    "plt.savefig(\"Gamewinner_time_tuuned_real\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "#plt.plot([item[3] for item in Model_performances],label=('Adaboost'))\n",
    "error=errors(error_xg)\n",
    "plt.plot(np.array(val)*100,label=('XG_test'))\n",
    "plt.fill_between(np.arange(0,len(error)), np.array(val)*100-error, np.array(val)*100+error,alpha=0.5)\n",
    "plt.plot(np.full(14, 80.5),label=(\"BaseLine\"),linestyle=(\"dashed\"),color=\"black\")\n",
    "plt.title('Model Accuracy for each Game time_stamp')\n",
    "plt.xticks(range(len(error)),np.arange(1,len(error)+1,1))\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlabel('Time Stamps (Point number in game)')\n",
    "plt.ylabel('Accuracy score [%]');\n",
    "print(f\"XG_Test {round(np.mean(np.array(val)*100),2)}\")\n",
    "#\\n---Average Accuracy--- \\nAdaboost: {round(np.mean([item[3] for item in Model_performances]),2)}\n",
    "plt.savefig(\"Gamewinner_time_tuuned_real\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(FI_XGB_final)):\n",
    "    plot_feature_importance(FI_XGB_final[i],labels,\"XG\")\n",
    "    plt.title(f\"Timestamp {i+1}\")\n",
    "    plt.savefig(f\"Game_winner_feature{i+1}\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under Her der model for timestamp 4 + overall model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time=df[df['Game_timestamp']==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=df[(df['P1Score']==15) & (df['P2Score']==40)]\n",
    "df_1_val=df_val[(df_val['P1Score']==15) & (df_val['P2Score']==40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2=df[(df['P1Score']==30) & (df['P2Score']==30)]\n",
    "df_2_val=df_val[(df_val['P1Score']==30) & (df_val['P2Score']==30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3=df[(df['P1Score']==40) & (df['P2Score']==15)]\n",
    "df_3_val=df_val[(df_val['P1Score']==40) & (df_val['P2Score']==15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=[df_1,df_2,df_3]\n",
    "dfs_val=[df_1_val,df_2_val,df_3_val]\n",
    "scores=[\"15-40\",\"30-30\",\"40-15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines=[]\n",
    "for i in range(len(dfs)):\n",
    "    baselines.append(dfs[i]['PointWinner'].value_counts()[0]/len(dfs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for 15-40, 30-30, 40-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "RFC_features=[]\n",
    "XGB_features=[]\n",
    "LR_features=[]\n",
    "val=[]\n",
    "ADA_features=[]\n",
    "Model_performances=[]\n",
    "\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    y=dfs[i]['GameWinnerA'].values\n",
    "    x=dfs[i][Game_test_columns].values\n",
    "    y_val=dfs_val[i]['GameWinnerA'].values\n",
    "    x_val=dfs_val[i][Game_test_columns].values\n",
    "    labels=Game_test_columns\n",
    "    labels=Game_test_columns\n",
    "    scaler = StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    x_val=scaler.transform(x_val)\n",
    "    folds = 10\n",
    "    kf = KFold(n_splits=folds, shuffle=True)\n",
    "    RFCScore = []\n",
    "    ADAScore = []\n",
    "    XGBScore = []\n",
    "    LRScore=[]\n",
    "    FI_rfc = []\n",
    "    FI_ada = []\n",
    "    FI_XGB = []\n",
    "    FI_LR = []\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)  \n",
    "    model_xg = XGBClassifier(objective ='binary:logistic', colsample_bytree = 1, learning_rate = 0.1,\n",
    "                max_depth = 6, verbosity=0)\n",
    "    model_xg.fit(X_train, y_train);\n",
    "    y_pred_xg = model_xg.predict(X_test);\n",
    "    scoreXG = accuracy_score(y_pred_xg, y_test)\n",
    "        #print(f\"Model training score {round(scoreXG,2)}\")\n",
    "    val_pred=model_xg.predict(x_val)\n",
    "    val_scoreXG = accuracy_score(val_pred, y_val)\n",
    "        #print(f\"Validation set score {round(val_scoreXG,3)}\")\n",
    "    val.append(val_scoreXG) \n",
    "    \n",
    "    for train_index, test_index in kf.split(x):\n",
    "        m=[]\n",
    "\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        RFC = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "        predRFC = RFC.fit(X_train,Y_train)\n",
    "        y_predRFC = predRFC.predict(X_test)\n",
    "        scoreRFC = accuracy_score(y_predRFC, Y_test)\n",
    "        RFCScore.append(scoreRFC)\n",
    "        FI_rfc.append(RFC.feature_importances_)\n",
    "        \n",
    "        #XG\n",
    "        model_xg = XGBClassifier(objective ='binary:logistic', colsample_bytree = 1, learning_rate = 0.1,\n",
    "                max_depth = 6, verbosity=0)\n",
    "        model_xg.fit(X_train, Y_train);\n",
    "        y_pred_xg = model_xg.predict(X_test);\n",
    "        scoreXG = accuracy_score(y_pred_xg, Y_test)\n",
    "        XGBScore.append(scoreXG)\n",
    "        FI_XGB.append(model_xg.feature_importances_)       \n",
    "        \n",
    "                #LR\n",
    "        logreg = LogisticRegression()\n",
    "        logreg.fit(X_train, Y_train)\n",
    "        y_pred_lr=logreg.predict(X_test)\n",
    "        scoreLR=accuracy_score(y_pred_lr, Y_test)\n",
    "        LRScore.append(scoreLR)\n",
    "            #FI_LR.append(logreg.feature_importances_)\n",
    "    \n",
    "    \n",
    "    XGB_features.append(list(map(mean, zip(*FI_XGB))))\n",
    "    RFC_features.append(list(map(mean, zip(*FI_rfc))))\n",
    "    ADA_features.append(list(map(mean, zip(*FI_ada))))\n",
    "    #LR_features.append(list(map(mean, zip(*FI_lr))))\n",
    "    #rf_scores.append(np.mean(RFCScore)*100)\n",
    "    #lr_scores.append(np.mean(LRScore)*100)\n",
    "    #xg_scores.append(np.mean(XGBScore)*100)\n",
    "    m.append(np.mean(RFCScore)*100)\n",
    "    m.append(np.mean(XGBScore)*100)\n",
    "    m.append(np.mean(LRScore)*100)\n",
    "    m.append(np.mean(ADAScore)*100)\n",
    "        \n",
    "    Model_performances.append(m)\n",
    "    print(f\"\\nModel Accuracy for timestamp {scores[i]}                    \")\n",
    "    print(f\"Random Forest: {np.round(np.mean(RFCScore)*100,2)}% ---- XGboost: {np.round(np.mean(XGBScore)*100,2)}% ---- Logistic Regression: {np.round(np.mean(LRScore)*100,2)}% \")\n",
    "    print(f\"XG_val: {round(val_scoreXG,2)*100}%\")\n",
    "        #---- XG: {np.round(np.mean(ADAScore)*100,2)}\n",
    "warnings.filterwarnings('default')\n",
    "plt.figure(figsize=(10,5))\n",
    "#plt.plot([item[3] for item in Model_performances],label=('Adaboost'))\n",
    "plt.plot(np.array(val)*100,label=('XG_val'))\n",
    "plt.plot([item[2] for item in Model_performances],label=('Logistic regression'))\n",
    "plt.plot([item[1] for item in Model_performances],label=('XGboost'))\n",
    "plt.plot([item[0] for item in Model_performances],label=('Random Forest'))\n",
    "plt.title('Model Accuracy for each time stamp')\n",
    "plt.axhline(baselines[0]*100,xmin=0,xmax=2,label=(\"BaseLine 15-40\"),linestyle=(\"dashed\"),color=\"r\")\n",
    "plt.axhline(baselines[1]*100,xmin=0,xmax=2,label=(\"BaseLine 30-30\"),linestyle=(\"dashed\"), color=\"b\")\n",
    "plt.axhline(baselines[2]*100,xmin=0,xmax=2,label=(\"BaseLine 40-15\"),linestyle=(\"dashed\"),color=\"black\")\n",
    "loc=range(len(scores))\n",
    "plt.xticks(loc, scores)\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Accuracy score at certain score of a game')\n",
    "plt.ylabel('Accuracy score [%]');\n",
    "print(f\"\\n---Average Accuracy--- \\nLogistic regression: {round(np.mean([item[2] for item in Model_performances]),2)}\\nXGboost {round(np.mean([item[1] for item in Model_performances]),2)}\\nRandom Forest {round(np.mean([item[0] for item in Model_performances]),2)}\")\n",
    "print(f\"XG_val {round(np.mean(np.array(val)*100),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(XGB_features)):\n",
    "    plot_feature_importance(XGB_features[i],labels,\"XG\")\n",
    "    plt.title(f\"score {scores[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One model  GameWinner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y=df['GameWinnerA'].values\n",
    "x=df[Game_test_columns].values\n",
    "labels=Game_test_columns\n",
    "scaler = StandardScaler()\n",
    "x=scaler.fit_transform(x)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "folds = 10\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "RFCScore = []\n",
    "adaScore = []\n",
    "XGBScore = []\n",
    "LRScore = []\n",
    "FI_rfc = []\n",
    "FI_ada = []\n",
    "FI_XGB = []\n",
    "\n",
    "i=0\n",
    "for train_index, test_index in kf.split(x):\n",
    "\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "\n",
    "    \n",
    "    #RF\n",
    "    \n",
    "    RFC = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "    predRFC = RFC.fit(X_train,Y_train)\n",
    "    y_predRFC = predRFC.predict(X_test)\n",
    "    scoreRFC = accuracy_score(y_predRFC, Y_test)\n",
    "    RFCScore.append(scoreRFC)\n",
    "    FI_rfc.append(RFC.feature_importances_)\n",
    "    \n",
    "    #ADA\n",
    "\n",
    "    #ada = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=2, random_state=0))\n",
    "    #predADA = ada.fit(X_train, Y_train)\n",
    "    #y_predADA = predADA.predict(X_test)\n",
    "    #scoreADA = accuracy_score(y_predADA, Y_test)\n",
    "    #adaScore.append(scoreADA)\n",
    "    #FI_ada.append(ada.feature_importances_)\n",
    "    \n",
    "    \n",
    "    #XG\n",
    "    eval_set = [(X_train, Y_train), (X_test, Y_test)]\n",
    "    model_xg = XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3,max_depth=6,\n",
    "                                     learning_rate = 0.1,verbosity=0)\n",
    "    model_xg.fit(X_train, Y_train, eval_metric=[\"error\", \"logloss\",\"auc\"],eval_set=eval_set );\n",
    "    y_pred_xg = model_xg.predict(X_test);\n",
    "    scoreXG = accuracy_score(y_pred_xg, Y_test)\n",
    "    XGBScore.append(scoreXG)\n",
    "    FI_XGB.append(model_xg.feature_importances_)\n",
    "    \n",
    "    #LR\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    y_pred_lr=logreg.predict(X_test)\n",
    "    scoreLR=accuracy_score(y_pred_lr, Y_test)\n",
    "    LRScore.append(scoreLR)\n",
    "    #FI_LR.append(logreg.feature_importances_)\n",
    "    \n",
    "    \n",
    "\n",
    "    i+=1\n",
    "    print(f\"Processing fold {i}\")\n",
    "warnings.filterwarnings('default')    \n",
    "rf_scores.append(np.mean(RFCScore)*100)\n",
    "xg_scores.append(np.mean(XGBScore)*100)\n",
    "print(\"\\nModel Accuracy\")\n",
    "print(f\"Baseline (Serve) 80.5% \") #Hvor mange gange vedkommende der server ogs vinder gamet\n",
    "print(f\"Random Forrest: {np.round(np.mean(RFCScore)*100,2)}%\")\n",
    "print(f\"XGboost: {np.round(np.mean(XGBScore)*100,2)}%\")\n",
    "#print(f\"ADAboost: {np.round(np.mean(adaScore)*100,2)}%\")\n",
    "print(f\"Logistic Regression: {np.round(np.mean(LRScore)*100,2)}%\")\n",
    "\n",
    "print(\"\\n Model feature impact > 0.01\")\n",
    "plot_feature_importance(list(map(mean, zip(*FI_rfc))),labels,\"Random Forrest\")\n",
    "plot_feature_importance(list(map(mean, zip(*FI_XGB))),labels,\"XG\")\n",
    "#plot_feature_importance(list(map(mean, zip(*FI_ada))),labels,\"Adaboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[Game_test_columns],df['GameWinnerA'].values, test_size=0.20, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) \n",
    "classifier=XGBClassifier(objective='binary:logistic')\n",
    "random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=10,scoring='accuracy',n_jobs=-1,cv=10,verbose=3)\n",
    "\n",
    "start_time = timer(None)\n",
    "random_search.fit(X_train,y_train)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable\n",
    "\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y=df['GameWinnerA'].values\n",
    "x=df[Game_test_columns].values\n",
    "y_val=df_val['GameWinnerA'].values\n",
    "x_val=df_val[Game_test_columns].values\n",
    "labels=Game_test_columns\n",
    "\n",
    "#x,y=sample(x,y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "x_val=scaler.transform(x_val);\n",
    "model_xg = XGBClassifier(objective ='binary:logistic',\n",
    "                         min_child_weight=1,\n",
    "                         gamma=0.1,\n",
    "                         colsample_bytree=0.9,\n",
    "                         sub_sample=0.5,\n",
    "                         max_depth=8,learning_rate = 0.08,\n",
    "                         verbosity=0,\n",
    "                         use_label_encoder=False,\n",
    "                         scale_pos_weight=1.3)\n",
    "\n",
    "\n",
    "model_xg.fit(X_train, y_train);\n",
    "y_pred_xg = model_xg.predict(X_test);\n",
    "scoreXG = accuracy_score(y_pred_xg, y_test)\n",
    "print(f\"Model training score {round(scoreXG,2)*100}\")\n",
    "\n",
    "val_pred=model_xg.predict(x_val)\n",
    "val_scoreXG = accuracy_score(val_pred, y_val)\n",
    "print(f\"Test set accuracy score {round(val_scoreXG,2)*100}%\")\n",
    "evaluation_time(y_val,val_pred,model_xg,x_val)\n",
    "plot_feature_importance(model_xg.feature_importances_,labels,\"XG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
