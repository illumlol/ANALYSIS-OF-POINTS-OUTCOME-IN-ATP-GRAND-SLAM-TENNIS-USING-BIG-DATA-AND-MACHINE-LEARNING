{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SetWinner Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "import plotly.graph_objects as go\n",
    "import pylab\n",
    "import warnings\n",
    "from termcolor import colored\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import *\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import json\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FinalData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_timestamp=[]\n",
    "\n",
    "k=0\n",
    "for i in range(len(df)):\n",
    "    if df.GameNo.iloc[i] != df.GameNo.iloc[i-1]:\n",
    "        k=1\n",
    "        game_timestamp.append(k)\n",
    "    if df.GameNo.iloc[i] == df.GameNo.iloc[i-1]:\n",
    "        k=k+1\n",
    "        game_timestamp.append(k)\n",
    "df['Game_timestamp']=game_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWFhWr1mjL-2"
   },
   "source": [
    "Feature Important function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyAVWvHEW0oW"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance,names,model_type):\n",
    "\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = importance\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    fi_df = fi_df[fi_df.feature_importance > 0.01]\n",
    "    \n",
    "\n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.tight_layout()\n",
    "    plt.yticks(fontsize=7)\n",
    "    #Plot Searborn bar chart\n",
    "    ax=sns.barplot(y='feature_names',x='feature_importance',data=fi_df,palette=\"rocket\")\n",
    "    ax.set_xlabel('feature_importance')\n",
    "    #plt.xticks(rotation = 90)\n",
    "    \n",
    "    #Add chart labels\n",
    "    plt.title(model_type + ' FEATURE IMPORTANCE')\n",
    "    plt.xlabel('FEATURE IMPORTANCE > 0.01')\n",
    "    plt.ylabel('FEATURE NAMES')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_time(y_val,y_pred,model,x_val):\n",
    "    plt.style.use('ggplot')\n",
    "    cm = confusion_matrix(y_val, (y_pred))\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax,fmt='g',cmap='rocket'); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted Winner');ax.set_ylabel('Actual Winner'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['Server', 'Returner']); ax.yaxis.set_ticklabels(['Server', 'Returner'], rotation=360);\n",
    "    plt.show()\n",
    "\n",
    "    f_score=f1_score(y_val, y_pred, average=None)\n",
    "    P_score=precision_score(y_val,y_pred,average=None)\n",
    "    recall=recall_score(y_val, y_pred, average=None)\n",
    "\n",
    "\n",
    "    ns_probs = [0 for _ in range(len(y_val))]\n",
    "    lr_probs = model.predict_proba(x_val)\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_val, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_val, lr_probs)\n",
    "    # summarize scores\n",
    "    print(f\"Test set accuracy score {(np.round(accuracy_score(y_pred,y_val),3)*100)}%\")\n",
    "    print(f\"F1 score {np.round(f_score,3)}\")\n",
    "    print(f\"Average F1 Score {np.round((f_score[0]+f_score[1])/2,3)}\")\n",
    "    print(f\"Recall score {np.round(recall,3)}\")\n",
    "    print(f\"Average Recall score {np.round((recall[0]+recall[1])/2,3)}\")\n",
    "    print(f\"Precision score {np.round(P_score,3)}\")\n",
    "    print(f\"Average precision Score {np.round((P_score[0]+P_score[1])/2,3)}\")\n",
    "    print('Model: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_val, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_val, lr_probs)\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Base')\n",
    "    plt.plot(lr_fpr, lr_tpr, marker='.', label='XGBoost')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def averages1(name,metrics):\n",
    "    \n",
    "    res = [round(sum(x) / len(x),3) for x in zip(*metrics)]\n",
    "    print(f\"{name} Accuracy score: {res[0]*100}, f1 score: {res[1]}, Precision: {res[2]}, Recall: {res[3]}, ROC-AUC: {res[4]}\")\n",
    "    return\n",
    "\n",
    "def metrics(Y_test,y_pred,x_val,model):\n",
    "    val_pred=y_pred\n",
    "    x_val=x_val\n",
    "    model=model\n",
    "    auc=round(accuracy_score(Y_test,val_pred),4)\n",
    "    f_score=round(f1_score(Y_test, val_pred),4)\n",
    "    P_score=round(precision_score(Y_test,val_pred),4)\n",
    "    recall=round(recall_score(Y_test, val_pred),4)\n",
    "    \n",
    "    ns_probs = [0 for _ in range(len(Y_test))]\n",
    "    lr_probs = model.predict_proba(x_val)\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(Y_test, ns_probs)\n",
    "    lr_auc = round(roc_auc_score(Y_test, lr_probs),4)\n",
    "    return [auc,f_score,P_score,recall,lr_auc]\n",
    "\n",
    "def averages(metrics):\n",
    "    \n",
    "    res = [round(sum(x) / len(x),3) for x in zip(*metrics)]\n",
    "    #print(f\"{name} Accuracy score: {res[0]*100}, f1 score: {res[1]}, Precision: {res[2]}, Recall: {res[3]}, ROC-AUC: {res[4]}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionV=['SetNo',\n",
    "                   'P1GamesWon',\n",
    "                   'P2GamesWon',\n",
    "                   'GameNo',\n",
    "                   'PointNumber',\n",
    "                   #'PointServer',\n",
    "                   'Tiebreak',\n",
    "                   'P1Score',\n",
    "                   'P2Score',\n",
    "                   'P1PointsWon',\n",
    "                   'P2PointsWon',\n",
    "                   #'Point_lenght_sec',\n",
    "                   'P1_ServeWidth_B_A',\n",
    "                   'P1_ServeWidth_BC_A',\n",
    "                   'P1_ServeWidth_BW_A',\n",
    "                   'P1_ServeWidth_C_A',\n",
    "                   'P1_ServeWidth_W_A',\n",
    "                   'P1_ServeDepth_CTL_A',\n",
    "                   'P1_ServeDepth_NCTL_A',\n",
    "                   'P1_ReturnDepth_D_A',\n",
    "                   'P1_ReturnDepth_ND_A',\n",
    "                   'P2_ServeWidth_B_A',\n",
    "                   'P2_ServeWidth_BC_A',\n",
    "                   'P2_ServeWidth_BW_A',\n",
    "                   'P2_ServeWidth_C_A',\n",
    "                   'P2_ServeWidth_W_A',\n",
    "                   'P2_ServeDepth_CTL_A',\n",
    "                   'P2_ServeDepth_NCTL_A',\n",
    "                   'P2_ReturnDepth_D_A',\n",
    "                   'P2_ReturnDepth_ND_A',\n",
    "                   'P1AceA',\n",
    "                   'P2AceA',\n",
    "                   'P1WinnerA',\n",
    "                   'P2WinnerA',\n",
    "                   'P1DoubleFaultA',\n",
    "                   'P2DoubleFaultA',\n",
    "                   'P1UnfErrA',\n",
    "                   'P2UnfErrA',\n",
    "                   'P1NetPointA',\n",
    "                   'P2NetPointA',\n",
    "                   'P1NetPointWonA',\n",
    "                   'P2NetPointWonA',\n",
    "                   'P1BreakPointA',\n",
    "                   'P2BreakPointA',\n",
    "                   'P1BreakPointWonA',\n",
    "                   'P2BreakPointWonA',\n",
    "                   'P1BreakPointMissedA',\n",
    "                   'P2BreakPointMissedA',\n",
    "                   'P1DistanceRunA',\n",
    "                   'P2DistanceRunA',\n",
    "                   'RallyCountA',\n",
    "                   'P1SetsWon',\n",
    "                   'P2SetsWon',\n",
    "                   'Game_timestamp',\n",
    "                   'P1Rank',\n",
    "                   'P2Rank',\n",
    "                   'Total_time',\n",
    "                   'Surface']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for making test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df):\n",
    "    matches=list(df.match_id.unique())\n",
    "    Test_games=random.sample(matches,round(len(matches)/10))\n",
    "    df_test=df[df['match_id'].isin(Test_games)]\n",
    "    df=df[~df['match_id'].isin(Test_games)]\n",
    "    return df,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,df_test=data_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2=[]\n",
    "for i in range(1,max(df.GameNo)):\n",
    "    l2.append(len(df[df.GameNo==i]))\n",
    "df_set=df[df['Game_timestamp']==1]\n",
    "print(l2)\n",
    "plt.plot(l2)\n",
    "plt.title('Number of rows for each time stamp (Games in a set) ')\n",
    "plt.xlabel('Game in a set')\n",
    "plt.ylabel('Number of rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test[df_test['Game_timestamp']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "RFC_features=[]\n",
    "XGB_features=[]\n",
    "LR_features=[]\n",
    "ADA_features=[]\n",
    "Model_performances=[]\n",
    "test=[]\n",
    "FI_XGB_final=[]\n",
    "RFF1 = []\n",
    "RFR = []\n",
    "RFP = []\n",
    "RFAU = []\n",
    "ADAF1 = []\n",
    "ADAR = []\n",
    "ADAP = []\n",
    "ADAAU = []\n",
    "XGF1 = []\n",
    "XGR = []\n",
    "XGP = []\n",
    "XGAU = []\n",
    "LRF1 = []\n",
    "LRR = []\n",
    "LRP = []\n",
    "LRAU = []\n",
    "\n",
    "\n",
    "f = open('RF_Hyper_time',)  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "RF_hyper_time = json.load(f)\n",
    "\n",
    "for i in range(13):\n",
    "    \n",
    "    if l2[i] > 3000:\n",
    "        y=df_set[df_set.GameNo==i+1]['SetWinnerA'].values\n",
    "        x=df_set[df_set.GameNo==i+1][predictionV].values\n",
    "        labels=predictionV\n",
    "        scaler = StandardScaler()\n",
    "        x=scaler.fit_transform(x)\n",
    "\n",
    "        folds = 10\n",
    "        kf = KFold(n_splits=folds, random_state=42, shuffle=True)\n",
    "        RFCScore = []\n",
    "        ADAScore = []\n",
    "        XGBScore = []\n",
    "        LRScore=[]\n",
    "        FI_rfc = []\n",
    "        FI_ada = []\n",
    "        FI_XGB = []\n",
    "        FI_LR = []\n",
    "        F_score = 0\n",
    "        p_score = 0\n",
    "        Recall = 0\n",
    "        lr_auc = 0\n",
    "        F2_score = 0\n",
    "        p2_score = 0\n",
    "        Recall2 = 0\n",
    "        lr_auc2 = 0\n",
    "        F3_score = 0\n",
    "        p3_score = 0\n",
    "        Recall3 = 0\n",
    "        lr_auc3 = 0\n",
    "        F4_score = 0\n",
    "        p4_score = 0\n",
    "        Recall4 = 0\n",
    "        lr_auc4 = 0\n",
    "        for train_index, val_index in kf.split(x):\n",
    "            m=[]\n",
    "\n",
    "            X_train, X_val = x[train_index], x[val_index]\n",
    "            Y_train, Y_val = y[train_index], y[val_index]\n",
    "\n",
    "\n",
    "        #RF\n",
    "\n",
    "            RFC = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "            RFC.fit(X_train,Y_train)\n",
    "            y_predRFC = RFC.predict(X_val)\n",
    "            scoreRFC = accuracy_score(y_predRFC, Y_val)\n",
    "            RFCScore.append(scoreRFC)\n",
    "            FI_rfc.append(RFC.feature_importances_)\n",
    "            f_score = f1_score(Y_val, y_predRFC, average=None)\n",
    "            F_score = F_score + np.round((f_score[0]+f_score[1])/2,3)\n",
    "            P_score=precision_score(Y_val,y_predRFC,average=None)\n",
    "            p_score = p_score + np.round((P_score[0]+P_score[1])/2,3)\n",
    "            recall=recall_score(Y_val, y_predRFC, average=None)\n",
    "            Recall = Recall + np.round((recall[0]+recall[1])/2,3)\n",
    "            lr_probs = RFC.predict_proba(X_val)\n",
    "            lr_probs = lr_probs[:, 1]\n",
    "            lr_auc = lr_auc + roc_auc_score(Y_val, lr_probs)\n",
    "\n",
    "        #ADA\n",
    "\n",
    "            ada = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=6, random_state=0))\n",
    "            ada.fit(X_train, Y_train)\n",
    "            y_predADA = ada.predict(X_val)\n",
    "            scoreADA = accuracy_score(y_predADA, Y_val)\n",
    "            ADAScore.append(scoreADA)\n",
    "            FI_ada.append(ada.feature_importances_)\n",
    "            f_score = f1_score(Y_val, y_predADA, average=None)\n",
    "            F2_score = F2_score + np.round((f_score[0]+f_score[1])/2,3)\n",
    "            P_score=precision_score(Y_val,y_predADA,average=None)\n",
    "            p2_score = p2_score + np.round((P_score[0]+P_score[1])/2,3)\n",
    "            recall=recall_score(Y_val, y_predADA, average=None)\n",
    "            Recall2 = Recall2 + np.round((recall[0]+recall[1])/2,3)\n",
    "            lr_probs = ada.predict_proba(X_val)\n",
    "            lr_probs = lr_probs[:, 1]\n",
    "            lr_auc2 = lr_auc2 + roc_auc_score(Y_val, lr_probs)\n",
    "\n",
    "\n",
    "        #XG\n",
    "            model_xg = XGBClassifier(objective ='binary:logistic',min_child_weight=2,gamma=0.2,colsample_bytree=0.3,\n",
    "                                     max_depth=6,learning_rate = 0.05,verbosity=0,use_label_encoder=False)\n",
    "            model_xg.fit(X_train, Y_train);\n",
    "            y_pred_xg = model_xg.predict(X_val);\n",
    "            scoreXG = accuracy_score(y_pred_xg, Y_val)\n",
    "            XGBScore.append(scoreXG)\n",
    "            FI_XGB.append(model_xg.feature_importances_)\n",
    "            f_score = f1_score(Y_val, y_pred_xg, average=None)\n",
    "            F3_score = F3_score + np.round((f_score[0]+f_score[1])/2,3)\n",
    "            P_score=precision_score(Y_val,y_pred_xg,average=None)\n",
    "            p3_score = p3_score + np.round((P_score[0]+P_score[1])/2,3)\n",
    "            recall=recall_score(Y_val, y_pred_xg, average=None)\n",
    "            Recall3 = Recall3 + np.round((recall[0]+recall[1])/2,3)\n",
    "            lr_probs = model_xg.predict_proba(X_val)\n",
    "            lr_probs = lr_probs[:, 1]\n",
    "            lr_auc3 = lr_auc3 + roc_auc_score(Y_val, lr_probs)\n",
    "        \n",
    "        #LR\n",
    "            logreg = LogisticRegression()\n",
    "            logreg.fit(X_train, Y_train)\n",
    "            y_pred_lr=logreg.predict(X_val)\n",
    "            scoreLR=accuracy_score(y_pred_lr, Y_val)\n",
    "            LRScore.append(scoreLR)\n",
    "            #FI_LR.append(logreg.feature_importances_)\n",
    "            f_score = f1_score(Y_val, y_pred_lr, average=None)\n",
    "            F4_score = F4_score + np.round((f_score[0]+f_score[1])/2,3)\n",
    "            P_score=precision_score(Y_val,y_pred_lr,average=None)\n",
    "            p4_score = p4_score + np.round((P_score[0]+P_score[1])/2,3)\n",
    "            recall=recall_score(Y_val, y_pred_lr, average=None)\n",
    "            Recall4 = Recall4 + np.round((recall[0]+recall[1])/2,3)\n",
    "            lr_probs = logreg.predict_proba(X_val)\n",
    "            lr_probs = lr_probs[:, 1]\n",
    "            lr_auc4 = lr_auc4 + roc_auc_score(Y_val, lr_probs)\n",
    "\n",
    "    \n",
    "    #i+=1\n",
    "    #print(f\"Processing fold {i}\")\n",
    "        XGB_features.append(list(map(mean, zip(*FI_XGB))))\n",
    "        RFC_features.append(list(map(mean, zip(*FI_rfc))))\n",
    "        ADA_features.append(list(map(mean, zip(*FI_ada))))\n",
    "        #LR_features.append(list(map(mean, zip(*FI_lr))))\n",
    "        #rf_scores.append(np.mean(RFCScore)*100)\n",
    "        #lr_scores.append(np.mean(LRScore)*100)\n",
    "        #xg_scores.append(np.mean(XGBScore)*100)\n",
    "        m.append(np.mean(RFCScore)*100)\n",
    "        m.append(np.mean(XGBScore)*100)\n",
    "        m.append(np.mean(LRScore)*100)\n",
    "        m.append(np.mean(ADAScore)*100)\n",
    "        RFF1.append(F_score/10)\n",
    "        RFR.append(Recall/10)\n",
    "        RFP.append(p_score/10)\n",
    "        RFAU.append(lr_auc/10)\n",
    "        ADAF1.append(F2_score/10)\n",
    "        ADAR.append(Recall2/10)\n",
    "        ADAP.append(p2_score/10)\n",
    "        ADAAU.append(lr_auc2/10)\n",
    "        XGF1.append(F3_score/10)\n",
    "        XGR.append(Recall3/10)\n",
    "        XGP.append(p3_score/10)\n",
    "        XGAU.append(lr_auc3/10)\n",
    "        LRF1.append(F4_score/10)\n",
    "        LRR.append(Recall4/10)\n",
    "        LRP.append(p4_score/10)\n",
    "        LRAU.append(lr_auc4/10)\n",
    "        \n",
    "        Model_performances.append(m)\n",
    "    \n",
    "        print(f\"\\nModel Accuracy for timestamp {i+1} \") \n",
    "        print(f\"Random Forest: {np.round(np.mean(RFCScore)*100,2)}% ---- XGboost: {np.round(np.mean(XGBScore)*100,2)}% ---- Logistic Regression: {np.round(np.mean(LRScore)*100,2)}%,---- Adaboost: {np.round(np.mean(ADAScore)*100,2)}\")\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot([item[3] for item in Model_performances],label=('Adaboost'), color = \"red\")\n",
    "plt.plot([item[2] for item in Model_performances],label=('Logistic regression'), color = 'Purple')\n",
    "plt.plot([item[1] for item in Model_performances],label=('XGboost'), color = 'yellow')\n",
    "plt.plot([item[0] for item in Model_performances],label=('Random Forest'), color = 'blue')\n",
    "plt.plot(np.full(len(Model_performances), 61.14),label=(\"BaseLine\"),linestyle=(\"dashed\"), color = 'black')\n",
    "plt.title('Model Accuracy for each timestamp validation set')\n",
    "plt.xticks(range(0,len(XGB_features)),labels=np.arange(1,len(XGB_features)+1))\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlabel('Time Stamps (Game in set)')\n",
    "plt.ylabel('Accuracy score [%]');\n",
    "plt.savefig(\"SetWinnerPlotVal\",bbox_inches='tight',dpi=300)\n",
    "print(f\"\\n---Average Accuracy --- \\n Adaboost {round(np.mean([item[3] for item in Model_performances]),2)}\\nLogistic regression: {round(np.mean([item[2] for item in Model_performances]),2)}\\nXGboost {round(np.mean([item[1] for item in Model_performances]),2)}\\nRandom Forest {round(np.mean([item[0] for item in Model_performances]),2)}\\nRF_test {round(np.mean(test)*100,2)}\")\n",
    "print(f\"\\n---Average f1-score ---\\n Adaboost {round(np.mean(ADAF1)*100,2)}\\nLogistic regression: {round(np.mean(LRF1)*100,2)}\\nXGboost {round(np.mean(XGF1)*100,2)}\\nRandom Forest {round(np.mean(RFF1)*100,2)}\")\n",
    "print(f\"\\n---Average Recall score ---\\n Adaboost {round(np.mean(ADAR)*100,2)}\\nLogistic regression: {round(np.mean(LRR)*100,2)}\\nXGboost {round(np.mean(XGR)*100,2)}\\nRandom Forest {round(np.mean(RFR)*100,2)}\")\n",
    "print(f\"\\n---Average Precision score ---\\n Adaboost {round(np.mean(ADAP)*100,2)}\\nLogistic regression: {round(np.mean(LRP)*100,2)}\\nXGboost {round(np.mean(XGP)*100,2)}\\nRandom Forest {round(np.mean(RFP)*100,2)}\")\n",
    "print(f\"\\n---Average ROC-AUC ---\\n Adaboost {round(np.mean(ADAAU)*100,2)}\\nLogistic regression: {round(np.mean(LRAU)*100,2)}\\nXGboost {round(np.mean(XGAU)*100,2)}\\nRandom Forest {round(np.mean(RFAU)*100,2)}\")\n",
    "#\\n---Average Accuracy --- \\n Adaboost {round(np.mean([item[3] for item in Model_performances]),2)}\n",
    "#---- Adaboost: {np.round(np.mean(ADAScore)*100,2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "params={\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7, 9 ],\n",
    " \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"colsample_bytree\" : [ 0.3, 0.6, 0.9, 1.2 , 1.5 ],\n",
    " \"subsample\": np.arange(0.5, 1.0, 0.11).tolist(),\n",
    " \"max_depth\"        : [ 4, 5, 6, 8, 10, 12],\n",
    " \"learning_rate\"    : [0.01, 0.05, 0.08, 0.1],\n",
    " \"scale_pos_weight\":np.arange(1, 1.5, 0.1).tolist()\n",
    "}\n",
    "\n",
    "\n",
    "XG_hyper_time=[]\n",
    "for i in range(len(Model_performances)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_set[df_set.GameNo==i+1][predictionV].values,df_set[df_set.GameNo==i+1]['SetWinnerA'].values, test_size=0.20, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test) \n",
    "    XG = XGBClassifier(objective='binary:logistic')\n",
    "    #grid_search=GridSearchCV(classifier,param_grid=params,scoring='accuracy',n_jobs=-1,cv=10)\n",
    "\n",
    "    #start_time = timer(None)\n",
    "    #grid_search.fit(X_train,y_train)\n",
    "    #timer(start_time) # timing ends here for \"start_time\" variable\n",
    "    \n",
    "    random_search=RandomizedSearchCV(XG,param_distributions=params,n_iter=25,scoring='accuracy',n_jobs=-1,cv=10,verbose=3)\n",
    "    start_time = timer(None)\n",
    "    random_search.fit(X_train,y_train)\n",
    "    timer(start_time) # timing ends here for \"start_time\" variable\n",
    "    \n",
    "    \n",
    "    XG_hyper_time.append(random_search.best_params_)\n",
    "with open('XG_Hyper_time', 'w') as fout:\n",
    "    json.dump(XG_hyper_time, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "RFC_features=[]\n",
    "XGB_features=[]\n",
    "LR_features=[]\n",
    "ADA_features=[]\n",
    "Model_performances=[]\n",
    "test=[]\n",
    "FI_XGB_final=[]\n",
    "error_xg=[]\n",
    "\n",
    "#f = open('RF_Hyper_time',)  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "#RF_hyper_time = json.load(f)\n",
    "\n",
    "f = open('XG_Hyper_time')\n",
    "XG_Hyper_time = json.load(f)\n",
    "\n",
    "for i in range(13):\n",
    "    \n",
    "    if l2[i] > 3000:\n",
    "        y=df_set[df_set.GameNo==i+1]['SetWinnerA'].values\n",
    "        x=df_set[df_set.GameNo==i+1][predictionV].values\n",
    "        y_test=df_test[df_test.GameNo==i+1]['SetWinnerA'].values\n",
    "        x_test=df_test[df_test.GameNo==i+1][predictionV].values\n",
    "        labels=predictionV\n",
    "        scaler = StandardScaler()\n",
    "        x=scaler.fit_transform(x)\n",
    "        x_test=scaler.transform(x_test)\n",
    "\n",
    "        RFCScore = []\n",
    "        ADAScore = []\n",
    "        XGBScore = []\n",
    "        LRScore=[]\n",
    "        FI_rfc = []\n",
    "        FI_ada = []\n",
    "        FI_XGB = []\n",
    "        FI_LR = []\n",
    "        error_fill=[]\n",
    "        for j in range(10):\n",
    "            df_xg,df_test_xg=data_split(df)\n",
    "            y2=df_xg[df_xg.Game_timestamp==i+1]['SetWinnerA'].values\n",
    "            x2=df_xg[df_xg.Game_timestamp==i+1][predictionV].values\n",
    "            y_test2=df_test_xg[df_test_xg.Game_timestamp==i+1]['SetWinnerA'].values\n",
    "            x_test2=df_test_xg[df_test_xg.Game_timestamp==i+1][predictionV].values\n",
    "            #x, y = sample(x,y)\n",
    "            labels=predictionV\n",
    "            x2=scaler.transform(x2)\n",
    "            x_test2=scaler.transform(x_test2)\n",
    "            model_xg2 = XGBClassifier(objective ='binary:logistic',min_child_weight=XG_Hyper_time[i]['min_child_weight'],gamma=XG_Hyper_time[i]['gamma'],colsample_bytree=XG_Hyper_time[i]['colsample_bytree'],\n",
    "                                     max_depth=XG_Hyper_time[i]['max_depth'],learning_rate = XG_Hyper_time[i]['learning_rate'],verbosity=0,use_label_encoder=False, scale_pos_weight = XG_Hyper_time[i]['scale_pos_weight'],\n",
    "                                subsample = XG_Hyper_time[i]['subsample'])\n",
    "            #model_xg = XGBClassifier(objective ='binary:logistic',params=params_game[i])\n",
    "            model_xg2.fit(x2, y2);\n",
    "            error_pred=model_xg2.predict(x_test2)\n",
    "            error_scoreXG = accuracy_score(error_pred, y_test2)\n",
    "            error_fill.append(error_scoreXG)\n",
    "    \n",
    "        #X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.20, random_state=42)\n",
    "        \n",
    "        model_xg = XGBClassifier(objective ='binary:logistic',min_child_weight=XG_Hyper_time[i]['min_child_weight'],gamma=XG_Hyper_time[i]['gamma'],colsample_bytree=XG_Hyper_time[i]['colsample_bytree'],\n",
    "                                     max_depth=XG_Hyper_time[i]['max_depth'],learning_rate = XG_Hyper_time[i]['learning_rate'],verbosity=0,use_label_encoder=False, scale_pos_weight = XG_Hyper_time[i]['scale_pos_weight'],\n",
    "                                subsample = XG_Hyper_time[i]['subsample'])\n",
    "        #model_xg = RandomForestClassifier(n_estimators = RF_hyper_time[i]['n_estimators'],min_samples_split = RF_hyper_time[i]['min_samples_split'],\n",
    "         #                               min_samples_leaf = RF_hyper_time[i]['min_samples_leaf'],max_features = RF_hyper_time[i]['max_features'],\n",
    "          #                               max_depth = RF_hyper_time[i]['max_depth'],bootstrap = RF_hyper_time[i]['bootstrap'] )\n",
    "        model_xg.fit(x, y);\n",
    "        #y_pred_xg = model_xg.predict(X_test);\n",
    "        #scoreXG = accuracy_score(y_pred_xg, y_test)\n",
    "        #print(f\"Model training score {round(scoreXG,2)}\")\n",
    "        test_pred=model_xg.predict(x_test)\n",
    "        test_scoreXG = accuracy_score(test_pred, y_test)\n",
    "        #print(f\"Validation set score {round(val_scoreXG,3)}\")\n",
    "        error_xg.append(np.mean(error_fill))\n",
    "        test.append(test_scoreXG)\n",
    "        FI_XGB_final.append(model_xg.feature_importances_)\n",
    "        print(f\"Predicted dist: {np.unique(test_pred, return_counts=True)}\")\n",
    "        print(f\"Actual Dist: {np.unique(y_test,return_counts=True)}\")\n",
    "    #i+=1\n",
    "    #print(f\"Processing fold {i}\")\n",
    "        XGB_features.append(list(map(mean, zip(*FI_XGB))))\n",
    "        RFC_features.append(list(map(mean, zip(*FI_rfc))))\n",
    "        ADA_features.append(list(map(mean, zip(*FI_ada))))\n",
    "        #LR_features.append(list(map(mean, zip(*FI_lr))))\n",
    "        #rf_scores.append(np.mean(RFCScore)*100)\n",
    "        #lr_scores.append(np.mean(LRScore)*100)\n",
    "        #xg_scores.append(np.mean(XGBScore)*100)\n",
    "        print(f\"Timestamp {i+1}\")\n",
    "        evaluation_time(y_test,test_pred,model_xg,x_test)\n",
    "        \n",
    "print(f\"--- Average Accuracy--- \\nXG_test {round(np.mean(test)*100,2)}\")\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(np.array(test)*100,label=('XG_test'))\n",
    "plt.fill_between(np.arange(0,len(error_xg)), np.array(test)*100-error_xg, np.array(test)*100+error_xg,alpha=0.5)\n",
    "plt.plot(np.full(len(test), 61.14),label=(\"BaseLine\"),linestyle=(\"dashed\"))\n",
    "plt.title('Model Accuracy for each timestamp for XGBoost on test data')\n",
    "plt.xticks(range(0,len(XGB_features)),labels=np.arange(1,len(XGB_features)+1))\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlabel('Timestamps (Game in set)')\n",
    "plt.ylabel('Accuracy score [%]');\n",
    "plt.savefig(\"SetWinnerPlotTest\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(FI_XGB_final[5],labels,\"XG\")\n",
    "plt.title(f\"Set_timestamp (GameNo) {6}\")\n",
    "plt.savefig(\"FI_8\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(FI_XGB_final[10],labels,\"XG\")\n",
    "plt.title(f\"Set_timestamp (GameNo) {11}\")\n",
    "plt.savefig(\"FI_11\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(FI_XGB_final)):\n",
    "    plot_feature_importance(FI_XGB_final[i],labels,\"XG\")\n",
    "    plt.title(f\"Set_timestamp (GameNo) {i+1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
