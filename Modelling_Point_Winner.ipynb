{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLZv4nzvQN8w"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "import plotly.graph_objects as go\n",
    "import pylab\n",
    "import warnings\n",
    "from scipy.stats import sem\n",
    "from datetime import datetime\n",
    "#from termcolor import colored\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold, KFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.2 MB 470 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tenacity>=6.2.0\n",
      "  Using cached tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.10.0 tenacity-8.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pflXMP7hQN80"
   },
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJNO-gYMQN80"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('Akkumulated_tennis_PR.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NVvcq12QN80",
    "scrolled": false
   },
   "source": [
    "df=df.drop(['Unnamed: 0.1',\n",
    " 'Unnamed: 0',\n",
    " 'P1Ace',\n",
    " 'P2Ace',\n",
    " 'P1Winner',\n",
    " 'P2Winner',\n",
    " 'P1DoubleFault',\n",
    " 'P2DoubleFault',\n",
    " 'P1UnfErr',\n",
    " 'P2UnfErr',\n",
    " 'P1NetPoint',\n",
    " 'P2NetPoint',\n",
    " 'P1NetPointWon',\n",
    " 'P2NetPointWon',\n",
    " 'P1BreakPoint',\n",
    " 'P2BreakPoint',\n",
    " 'P1BreakPointWon',\n",
    " 'P2BreakPointWon',\n",
    " 'P1BreakPointMissed',\n",
    " 'P2BreakPointMissed',\n",
    " 'ServeIndicator',\n",
    " 'ServeNumber',\n",
    " 'WinnerType',\n",
    " 'WinnerShotType',\n",
    " 'P1DistanceRun',\n",
    " 'P2DistanceRun',\n",
    " 'RallyCount','P1_ServeWidth_0',\n",
    " 'P1_ServeWidth_B',\n",
    " 'P1_ServeWidth_BC',\n",
    " 'P1_ServeWidth_BW',\n",
    " 'P1_ServeWidth_C',\n",
    " 'P1_ServeWidth_DoubleFault',\n",
    " 'P1_ServeWidth_W',\n",
    " 'P1_ServeDepth_0',\n",
    " 'P1_ServeDepth_CTL',\n",
    " 'P1_ServeDepth_DoubleFault',\n",
    " 'P1_ServeDepth_NCTL',\n",
    " 'P1_ReturnDepth_0',\n",
    " 'P1_ReturnDepth_D',\n",
    " 'P1_ReturnDepth_DoubleFault',\n",
    " 'P1_ReturnDepth_ND',\n",
    " 'P1_ReturnDepth_ServeAce',\n",
    " 'P1_ReturnDepth_Service box',\n",
    " 'P2_ServeWidth_0',\n",
    " 'P2_ServeWidth_B',\n",
    " 'P2_ServeWidth_BC',\n",
    " 'P2_ServeWidth_BW',\n",
    " 'P2_ServeWidth_C',\n",
    " 'P2_ServeWidth_DoubleFault',\n",
    " 'P2_ServeWidth_W',\n",
    " 'P2_ServeDepth_0',\n",
    " 'P2_ServeDepth_CTL',\n",
    " 'P2_ServeDepth_DoubleFault',\n",
    " 'P2_ServeDepth_NCTL',\n",
    " 'P2_ReturnDepth_0',\n",
    " 'P2_ReturnDepth_D',\n",
    " 'P2_ReturnDepth_DoubleFault',\n",
    " 'P2_ReturnDepth_ND',\n",
    " 'P2_ReturnDepth_ServeAce',\n",
    " 'P2_ReturnDepth_Service box',\n",
    " 'ElapsedTime',\n",
    " 'ServeWidth',\n",
    " 'ServeDepth',\n",
    " 'P1Momentum',\n",
    " 'P2Momentum',\n",
    " 'ReturnDepth',\n",
    " 'datetime',\n",
    " 'year',\n",
    " 'slam',\n",
    " 'match_num',\n",
    " 'player1',\n",
    " 'player2',\n",
    " 'tourney_name',\n",
    " 'tourney_date',\n",
    " 'winner_name',\n",
    " 'winner_hand',\n",
    " 'loser_name',\n",
    " 'loser_hand'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making rank for p1 and p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['match_id'] == \"2020-usopen-1228\", 'loser_rank'] = 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_rank = []\n",
    "p2_rank = []\n",
    "for i in range(len(df)):\n",
    "    if df.player1.iloc[i] == df.winner_name.iloc[i]:\n",
    "        p1_rank.append(df.winner_rank.iloc[i])\n",
    "        p2_rank.append(df.loser_rank.iloc[i])\n",
    "    if df.player1.iloc[i] == df.loser_name.iloc[i]:\n",
    "        p1_rank.append(df.loser_rank.iloc[i])\n",
    "        p2_rank.append(df.winner_rank.iloc[i])\n",
    "df['P1Rank'] = p1_rank\n",
    "df['P2Rank'] = p2_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making total elapsed time in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time=[]\n",
    "for i in range(len(df.match_id.unique())):\n",
    "    total_time = np.append(total_time, np.insert(df[df['match_id'] == df.match_id.unique()[i]].Point_lenght_sec.cumsum().values[:-1],0,0))\n",
    "df['Total_time']=total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a tiebreak variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to be shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_shifted=[\n",
    "    ('P1GamesWon','P2GamesWon'),\n",
    "    ('P1Score','P2Score'),\n",
    "    ('P1PointsWon','P2PointsWon'),\n",
    "    ('P1_ServeWidth_B_A','P2_ServeWidth_B_A'),\n",
    "    ('P1_ServeWidth_BC_A','P2_ServeWidth_BC_A'),\n",
    "    ('P1_ServeWidth_BW_A','P2_ServeWidth_BW_A'),\n",
    "    ('P1_ServeWidth_C_A','P2_ServeWidth_C_A'),\n",
    "    ('P1_ServeWidth_W_A','P2_ServeWidth_W_A'),\n",
    "    ('P1_ServeDepth_CTL_A','P2_ServeDepth_CTL_A'),\n",
    "    ('P1_ServeDepth_NCTL_A','P2_ServeDepth_NCTL_A'),\n",
    "    ('P1_ReturnDepth_D_A','P2_ReturnDepth_D_A'),\n",
    "    ('P1_ReturnDepth_ND_A','P2_ReturnDepth_ND_A'),\n",
    "    ('P1AceA','P2AceA'),\n",
    "    ('P1WinnerA','P2WinnerA'),\n",
    "    ('P1DoubleFaultA','P2DoubleFaultA'),\n",
    "    ('P1UnfErrA','P2UnfErrA'),\n",
    "    ('P1NetPointA','P2NetPointA'),\n",
    "    ('P1NetPointWonA','P2NetPointWonA'),\n",
    "    ('P1BreakPointA','P2BreakPointA'),\n",
    "    ('P1BreakPointWonA','P2BreakPointWonA'),\n",
    "    ('P1BreakPointMissedA','P2BreakPointMissedA'),\n",
    "    ('P1DistanceRunA','P1DistanceRunA'),\n",
    "    ('P1SetsWon','P2SetsWon'),\n",
    "    ('player1','player2'),\n",
    "    ('P1NetPoint','P2NetPoint'),\n",
    "    ('P1Rank','P2Rank')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing pointwinenr to binary encoding 0 = p1, 1= p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Server = {1: 0,2: 1}\n",
    "df.PointServer = [Server[item] for item in df.PointServer]\n",
    "df.PointWinner = [Server[item] for item in df.PointWinner]\n",
    "df.GameWinnerA = [Server[item] for item in df.GameWinnerA]\n",
    "df.SetWinnerA = [Server[item] for item in df.SetWinnerA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surface attribute, 0 = US open (Turf), 1 = Wimbeldon grass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface=[]\n",
    "for i in range(len(df)):\n",
    "    if df.match_id.iloc[i][5] == \"u\":\n",
    "        surface.append(0)\n",
    "    else:\n",
    "        surface.append(1)\n",
    "df['Surface']=surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TieBreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tie=[]\n",
    "for i in range(len(df)):\n",
    "    if (df.GameNo.iloc[i]>12) and (df.Surface.iloc[i]==0):\n",
    "        tie.append(1)\n",
    "    elif (df.GameNo.iloc[i] == 25) and (df.Surface.iloc[i]==1) and (df.year.iloc[i]>2018):\n",
    "        tie.append(1)\n",
    "    elif (df.GameNo.iloc[i] > 12) and (df.Surface.iloc[i]==1) and (df.SetNo.iloc[i] < 5):\n",
    "        tie.append(1)\n",
    "    else:\n",
    "        tie.append(0)\n",
    "df['Tiebreak']=tie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing point number mistakes in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V6llyDtQN81"
   },
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    if pd.isna(df.iloc[i]['PointNumber']):\n",
    "        df['PointNumber'].iloc[i]=df['PointNumber'].iloc[i+1]-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing NA value for loser rank in a single match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making unique time stamps for what point number in the game it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_timestamp=[]\n",
    "\n",
    "k=0\n",
    "for i in range(len(df)):\n",
    "    if df.GameNo.iloc[i] != df.GameNo.iloc[i-1]:\n",
    "        k=1\n",
    "        game_timestamp.append(k)\n",
    "    if df.GameNo.iloc[i] == df.GameNo.iloc[i-1]:\n",
    "        k=k+1\n",
    "        game_timestamp.append(k)\n",
    "df['Game_timestamp']=game_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game_test_columns=['SetNo',\n",
    "                   'P1GamesWon',\n",
    "                   'P2GamesWon',\n",
    "                   'GameNo',\n",
    "                   'PointNumber',\n",
    "                   'Tiebreak',\n",
    "                   'P1Score',\n",
    "                   'P2Score',\n",
    "                   'P1PointsWon',\n",
    "                   'P2PointsWon',\n",
    "                   'P1_ServeWidth_B_A',\n",
    "                   'P1_ServeWidth_BC_A',\n",
    "                   'P1_ServeWidth_BW_A',\n",
    "                   'P1_ServeWidth_C_A',\n",
    "                   'P1_ServeWidth_W_A',\n",
    "                   'P1_ServeDepth_CTL_A',\n",
    "                   'P1_ServeDepth_NCTL_A',\n",
    "                   'P1_ReturnDepth_D_A',\n",
    "                   'P1_ReturnDepth_ND_A',\n",
    "                   'P2_ServeWidth_B_A',\n",
    "                   'P2_ServeWidth_BC_A',\n",
    "                   'P2_ServeWidth_BW_A',\n",
    "                   'P2_ServeWidth_C_A',\n",
    "                   'P2_ServeWidth_W_A',\n",
    "                   'P2_ServeDepth_CTL_A',\n",
    "                   'P2_ServeDepth_NCTL_A',\n",
    "                   'P2_ReturnDepth_D_A',\n",
    "                   'P2_ReturnDepth_ND_A',\n",
    "                   'P1AceA',\n",
    "                   'P2AceA',\n",
    "                   'P1WinnerA',\n",
    "                   'P2WinnerA',\n",
    "                   'P1DoubleFaultA',\n",
    "                   'P2DoubleFaultA',\n",
    "                   'P1UnfErrA',\n",
    "                   'P2UnfErrA',\n",
    "                   'P1NetPointA',\n",
    "                   'P2NetPointA',\n",
    "                   'P1NetPointWonA',\n",
    "                   'P2NetPointWonA',\n",
    "                   'P1BreakPointA',\n",
    "                   'P2BreakPointA',\n",
    "                   'P1BreakPointWonA',\n",
    "                   'P2BreakPointWonA',\n",
    "                   'P1BreakPointMissedA',\n",
    "                   'P2BreakPointMissedA',\n",
    "                   'P1DistanceRunA',\n",
    "                   'P2DistanceRunA',\n",
    "                   'RallyCountA',\n",
    "                   'P1SetsWon',\n",
    "                   'P2SetsWon',\n",
    "                   'Game_timestamp',\n",
    "                   'P1Rank',\n",
    "                   'P2Rank',\n",
    "                   'Total_time',\n",
    "                   'Surface']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=0\n",
    "for i in range(len(df)-1):\n",
    "    if df2['P2GamesWon'].loc[i] > df2.P2GamesWon.loc[i+1] and df2['P2GamesWon'].loc[i+1] != 0:\n",
    "        k=k+1\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2[57924:57928]  Der er en fejl i P2GamesWOn her"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()\n",
    "for i in range(len(df)):\n",
    "    if df['PointServer'].loc[i] == 1:\n",
    "        for j in range(len(to_be_shifted)):\n",
    "            df.at[i,to_be_shifted[j][0]] = df2[to_be_shifted[j][1]].loc[i]\n",
    "            df.at[i,to_be_shifted[j][1]] = df2[to_be_shifted[j][0]].loc[i]\n",
    "        if df.PointWinner.loc[i] == 0:\n",
    "            df.at[i,\"PointWinner\"] = 1\n",
    "        elif df.PointWinner.loc[i] == 1:\n",
    "            df.at[i,\"PointWinner\"] = 0\n",
    "            \n",
    "        if df.GameWinnerA.loc[i] == 0:\n",
    "            df.at[i,\"GameWinnerA\"]=1\n",
    "        elif df.GameWinnerA.loc[i] == 1: \n",
    "            df.at[i,\"GameWinnerA\"] = 0\n",
    "\n",
    "        if df.SetWinnerA.loc[i] == 0:\n",
    "            df.at[i,\"SetWinnerA\"]=1\n",
    "        elif df.SetWinnerA.loc[i] == 1: \n",
    "            df.at[i,\"SetWinnerA\"] = 0\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(Y_test,y_pred,x_val,model):\n",
    "    val_pred=y_pred\n",
    "    x_val=x_val\n",
    "    model=model\n",
    "    auc=round(accuracy_score(Y_test,val_pred),4)\n",
    "    f_score=round(f1_score(Y_test, val_pred),4)\n",
    "    P_score=round(precision_score(Y_test,val_pred),4)\n",
    "    recall=round(recall_score(Y_test, val_pred),4)\n",
    "    \n",
    "    ns_probs = [0 for _ in range(len(Y_test))]\n",
    "    lr_probs = model.predict_proba(x_val)\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(Y_test, ns_probs)\n",
    "    lr_auc = round(roc_auc_score(Y_test, lr_probs),4)\n",
    "    return [auc,f_score,P_score,recall,lr_auc]\n",
    "def averages1(name,metrics):\n",
    "    \n",
    "    res = [round(sum(x) / len(x),3) for x in zip(*metrics)]\n",
    "    print(f\"{name} Accuracy score: {res[0]*100}, f1 score: {res[1]}, Precision: {res[2]}, Recall: {res[3]}, ROC-AUC: {res[4]}\\n\")\n",
    "    return\n",
    "\n",
    "def averages(metrics):\n",
    "    \n",
    "    res = [round(sum(x) / len(x),3) for x in zip(*metrics)]\n",
    "    #print(f\"{name} Accuracy score: {res[0]*100}, f1 score: {res[1]}, Precision: {res[2]}, Recall: {res[3]}, ROC-AUC: {res[4]}\")\n",
    "    return res\n",
    "\n",
    "import random\n",
    "def data_split(df):\n",
    "    matches=list(df.match_id.unique())\n",
    "    Val_games=random.sample(matches,round(len(matches)/10))\n",
    "    df_val=df[df['match_id'].isin(Val_games)]\n",
    "    df=df[~df['match_id'].isin(Val_games)]\n",
    "    return df,df_val\n",
    "\n",
    "\n",
    "def plot_feature_importance(importance,names,model_type):\n",
    "\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = importance\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    fi_df = fi_df[fi_df.feature_importance > 0.01]\n",
    "    \n",
    "\n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.tight_layout()\n",
    "    plt.yticks(fontsize=7)\n",
    "    #Plot Searborn bar chart\n",
    "    ax=sns.barplot(y='feature_names',x='feature_importance',data=fi_df,palette=\"rocket\")\n",
    "    ax.set_xlabel('feature_importance')\n",
    "    #plt.xticks(rotation = 90)\n",
    "    \n",
    "    #Add chart labels\n",
    "    plt.title(model_type + ' FEATURE IMPORTANCE')\n",
    "    plt.xlabel('FEATURE IMPORTANCE > 0.01')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "\n",
    "\n",
    "def evaluation_time(y_val,y_pred,model,x_val):\n",
    "    plt.style.use('ggplot')\n",
    "    cm = confusion_matrix(y_val, (val_pred))\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax,fmt='g',cmap='rocket'); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted Winner');ax.set_ylabel('Actual Winner'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['Server', 'Returner']); ax.yaxis.set_ticklabels(['Server', 'Returner'], rotation=360);\n",
    "    plt.show()\n",
    "\n",
    "    f_score=f1_score(y_val, val_pred, average=None)\n",
    "    P_score=precision_score(y_val,val_pred,average=None)\n",
    "    recall=recall_score(y_val, val_pred, average=None)\n",
    "\n",
    "\n",
    "    ns_probs = [0 for _ in range(len(y_val))]\n",
    "    lr_probs = model.predict_proba(x_val)\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_val, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_val, lr_probs)\n",
    "    # summarize scores\n",
    "    print(f\"Test set accuracy score {(np.round(accuracy_score(y_pred,y_val),3)*100)}%\")\n",
    "    print(f\"F1 score {np.round(f_score,3)}\")\n",
    "    print(f\"Recall score {np.round(recall,3)}\")\n",
    "    print(f\"Precision score {np.round(P_score,3)}\")\n",
    "    print('Model: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_val, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_val, lr_probs)\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Base')\n",
    "    plt.plot(lr_fpr, lr_tpr, marker='.', label='XGBoost')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluation(y_val,y_pred,model_xg,x_val):\n",
    "    plt.style.use('ggplot')\n",
    "    cm = confusion_matrix(y_val, (val_pred))\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax,fmt='g',cmap='rocket'); #annot=True to annotate cells\n",
    "\n",
    "    ax.set_xlabel('Predcited Winner');ax.set_ylabel('Actual Winner'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['Server', 'Returner']); ax.yaxis.set_ticklabels(['Server', 'Returner'], rotation=360);\n",
    "    plt.show()\n",
    "\n",
    "    f_score=f1_score(y_val, val_pred, average=None)\n",
    "    P_score=precision_score(y_val,val_pred,average=None)\n",
    "    recall=recall_score(y_val, val_pred, average=None)\n",
    "\n",
    "\n",
    "    ns_probs = [0 for _ in range(len(y_val))]\n",
    "    lr_probs = model_xg.predict_proba(x_val)\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_val, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_val, lr_probs)\n",
    "    # summarize scores\n",
    "    print(f\"Test set accuracy score {np.round(val_scoreXG*100,3)}%\")\n",
    "    print(f\"F1 score {np.round(f_score,3)}\")\n",
    "    print(f\"Recall score {np.round(recall,3)}\")\n",
    "    print(f\"Precision score {np.round(P_score,3)}\")\n",
    "    print('Xgboost: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_val, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_val, lr_probs)\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Base')\n",
    "    plt.plot(lr_fpr, lr_tpr, marker='.', label='XGBoost')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    results = model_xg.evals_result()\n",
    "    epochs = len(results['validation_0']['error'])\n",
    "    x_axis = range(0, epochs)\n",
    "    # plot log loss\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "    ax.legend()\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.title('XGBoost Log Loss')\n",
    "    plt.show()\n",
    "    # plot classification error\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "    ax.legend()\n",
    "    plt.ylabel('Classification Error')\n",
    "    plt.title('XGBoost Classification Error')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def errors(values):\n",
    "    errors=[]\n",
    "    for i in range(len(values)):\n",
    "        errors.append(sem(values[i])*100)\n",
    "    return errors\n",
    "\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "        \n",
    "def sample(x,y):\n",
    "    xx=np.unique(y,return_counts=True)\n",
    "    xx=xx[1][0]/xx[1][1]\n",
    "    if xx < 4:\n",
    "        return x,y\n",
    "    if xx > 4:\n",
    "        oversample = SMOTE(0.5)\n",
    "        x, y = oversample.fit_resample(x, y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Train and test set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df.copy()\n",
    "df,df_val=data_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1mJjFuDQN82"
   },
   "source": [
    "# Point Winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial model - RFC, XG, ADA, LOGREG, 10fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjJDxnYsQN82",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y=df['PointWinner'].values\n",
    "x=df[Game_test_columns]\n",
    "labels=list(x.columns)\n",
    "x=x.values\n",
    "scaler = StandardScaler()\n",
    "x=scaler.fit_transform(x)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "k=0\n",
    "for i in range(len(df)):\n",
    "    if df['PointWinner'].iloc[i] == df['PointServer'].iloc[i]:\n",
    "        k=k+1\n",
    "baseline=(k/len(df))*100\n",
    "\n",
    "\n",
    "folds = 10\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "RFCScore = []\n",
    "adaScore = []\n",
    "XGBScore = []\n",
    "LRScore = []\n",
    "FI_rfc = []\n",
    "FI_ada = []\n",
    "FI_XGB = []\n",
    "metrics_models=[]\n",
    "\n",
    "i=0\n",
    "for train_index, test_index in kf.split(x):\n",
    "\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "\n",
    "    \n",
    "    #RF\n",
    "    \n",
    "    RFC = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "    predRFC = RFC.fit(X_train,Y_train)\n",
    "    y_predRFC = predRFC.predict(X_test)\n",
    "    #scoreRFC = accuracy_score(y_predRFC, Y_test)\n",
    "    RFCScore.append(metrics(Y_test,y_predRFC,X_test,RFC))\n",
    "    FI_rfc.append(RFC.feature_importances_)\n",
    "    \n",
    "    #ADA\n",
    "\n",
    "    #ada = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=2, random_state=0))\n",
    "    #predADA = ada.fit(X_train, Y_train)\n",
    "    #y_predADA = predADA.predict(X_test)\n",
    "    #scoreADA = accuracy_score(y_predADA, Y_test)\n",
    "    #adaScore.append(metrics(Y_test,y_predADA,X_test,ada))\n",
    "    #FI_ada.append(ada.feature_importances_)\n",
    "    \n",
    "    \n",
    "    #XG\n",
    "    model_xg = XGBClassifier(objective ='binary:logistic')\n",
    "    model_xg.fit(X_train, Y_train);\n",
    "    y_pred_xg = model_xg.predict(X_test);\n",
    "    scoreXG = accuracy_score(y_pred_xg, Y_test)\n",
    "    XGBScore.append(metrics(Y_test,y_pred_xg,X_test,model_xg))\n",
    "    FI_XGB.append(model_xg.feature_importances_)\n",
    "    \n",
    "    #LR\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    y_pred_lr=logreg.predict(X_test)\n",
    "    LRScore.append(metrics(Y_test,y_pred_lr,X_test,logreg))\n",
    "    #FI_LR.append(logreg.feature_importances_)\n",
    "    \n",
    "\n",
    "    i+=1\n",
    "    print(f\"Processing fold {i}\")\n",
    "warnings.filterwarnings('default')    \n",
    "#rf_scores.append(np.mean(RFCScore)*100)\n",
    "#xg_scores.append(np.mean(XGBScore)*100)\n",
    "print(\"\\nModel Accuracy\")\n",
    "\n",
    "\n",
    "print(f\"Baseline 65%\")\n",
    "averages1(\"RFC\",RFCScore)\n",
    "averages1(\"XGBoost\",XGBScore)\n",
    "averages1(\"Logistic Regression\",LRScore)\n",
    "#averages1(\"AdaBoost\",adaScore)\n",
    "\n",
    "print(\"\\n Model feature impact > 0.01\")\n",
    "#plot_feature_importance(list(map(mean, zip(*FI_rfc))),labels,\"Random Forrest\")\n",
    "#plot_feature_importance(list(map(mean, zip(*FI_XGB))),labels,\"XG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining grid for XGBoost for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Values\n",
    "params={\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7, 9 ],\n",
    " \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"colsample_bytree\" : [ 0.3, 0.6, 0.9, 1.2 , 1.5 ],\n",
    " \"subsample\": np.arange(0.5, 1.0, 0.11).tolist(),\n",
    " \"max_depth\"        : [ 4, 5, 6, 8, 10, 12],\n",
    " \"learning_rate\"    : [0.01, 0.05, 0.08, 0.1],\n",
    " \"scale_pos_weight\":np.arange(1, 1.5, 0.1).tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunining pointwinner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasplit\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[Game_test_columns],df['PointWinner'].values, test_size=0.20, random_state=42)\n",
    "\n",
    "#Feature scaling\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Model for tuning\n",
    "classifier=XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "\n",
    "#RandomSearch\n",
    "random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=10,scoring='f1',n_jobs=-1,cv=10,verbose=3)\n",
    "start_time = timer(None)\n",
    "#random_search.fit(X_train,y_train)\n",
    "timer(start_time) \n",
    "\n",
    "#random_search.best_params_\n",
    "#Grid_Search\n",
    "#grid_search=GridSearchCV(classifier,param_grid=params,scoring='f1',n_jobs=-1,cv=10)\n",
    "#start_time = timer(None)\n",
    "#grid_search.fit(X_train,y_train)\n",
    "#timer(start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_child_weight=3,\n",
    "                         gamma=0.2,\n",
    "                         colsample_bytree=0.3,\n",
    "                         sub_sample=0.94,\n",
    "                          max_depth=5,\n",
    "                         learning_rate = 0.05,\n",
    "                         scale_pos_weight=1.1,\n",
    "                         verbosity=0,\n",
    "                         use_label_encoder=False,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param={'n_estimators': 1500,\n",
    " 'min_samples_split': 5,\n",
    " 'min_samples_leaf': 15,\n",
    " 'max_features': 'accuracy',\n",
    " 'max_depth': 10,\n",
    " 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[df.ServeNumber==1]['PointWinner'].values\n",
    "x=df[df.ServeNumber==1][Game_test_columns]\n",
    "y_val=df_val[df_val.ServeNumber==1]['PointWinner'].values\n",
    "x_val=df_val[df_val.ServeNumber==1][Game_test_columns].values\n",
    "labels=Game_test_columns\n",
    "\n",
    "#undersample = RandomUnderSampler(sampling_strategy=0.7)\n",
    "#x, y = undersample.fit_resample(x, y)\n",
    "#print(x.shape,y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "x_val=scaler.transform(x_val);\n",
    "\n",
    "\n",
    "model_xg = XGBClassifier(objective ='binary:logistic',min_child_weight=1,\n",
    "                         gamma=0.0,\n",
    "                         colsample_bytree=0.2,\n",
    "                         sub_sample=0.8,\n",
    "                          max_depth=5,\n",
    "                         learning_rate = 0.08,\n",
    "                         scale_pos_weight=1.6,\n",
    "                         verbosity=0,\n",
    "                         use_label_encoder=False,)\n",
    "\n",
    "model_xg.fit(X_train, y_train);\n",
    "y_pred_xg = model_xg.predict(X_test);\n",
    "scoreXG = accuracy_score(y_pred_xg, y_test)\n",
    "print(f\"Model training score {round(scoreXG,3)*100}\")\n",
    "val_pred=model_xg.predict(x_val)\n",
    "val_scoreXG = accuracy_score(val_pred, y_val)\n",
    "evaluation_time(y_val,val_pred,model_xg,x_val)\n",
    "plot_feature_importance(model_xg.feature_importances_,labels,\"XGBoost First serve\")\n",
    "plt.savefig(\"Pointwinner_S1_Features - XGboost\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[(df.ServeNumber==2) | (df.ServeNumber==0)]['PointWinner'].values\n",
    "x=df[(df.ServeNumber==2) | (df.ServeNumber==0)][Game_test_columns]\n",
    "y_val=df_val[(df_val.ServeNumber==2) | (df_val.ServeNumber==0)]['PointWinner'].values\n",
    "x_val=df_val[(df_val.ServeNumber==2) | (df_val.ServeNumber==0)][Game_test_columns].values\n",
    "labels=Game_test_columns\n",
    "\n",
    "#undersample = RandomUnderSampler(sampling_strategy=0.9)\n",
    "#x, y = undersample.fit_resample(x, y)\n",
    "#print(x.shape,y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "x_val=scaler.transform(x_val);\n",
    "model_xg = XGBClassifier(objective ='binary:logistic',min_child_weight=1,\n",
    "                         gamma=0.2,\n",
    "                         colsample_bytree=0.6,\n",
    "                         sub_sample=1,\n",
    "                          max_depth=6,\n",
    "                         learning_rate = 0.08,\n",
    "                         scale_pos_weight=1.0,\n",
    "                         verbosity=0,\n",
    "                         use_label_encoder=False,)\n",
    "\n",
    "\n",
    "model_xg.fit(X_train, y_train);\n",
    "y_pred_xg = model_xg.predict(X_test);\n",
    "scoreXG = accuracy_score(y_pred_xg, y_test)\n",
    "print(f\"Model training score {round(scoreXG,3)*100}\")\n",
    "\n",
    "val_pred=model_xg.predict(x_val)\n",
    "val_scoreXG = accuracy_score(val_pred, y_val)\n",
    "evaluation_time(y_val,val_pred,model_xg,x_val)\n",
    "plot_feature_importance(model_xg.feature_importances_,labels,\"XG Second Serve \")\n",
    "plt.savefig(\"Pointwinner_S2_Features - XGboost\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Serve study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game_test_columns=['SetNo',\n",
    "                   'P1GamesWon',\n",
    "                   'P2GamesWon',\n",
    "                   'GameNo',\n",
    "                   'PointNumber',\n",
    "                   #'PointServer',\n",
    "                   'Tiebreak',\n",
    "                   'P1Score',\n",
    "                   'P2Score',\n",
    "                   'P1PointsWon',\n",
    "                   'P2PointsWon',\n",
    "                   'Point_lenght_sec',\n",
    "                   'P1_ServeWidth_B_A',\n",
    "                   'P1_ServeWidth_BC_A',\n",
    "                   'P1_ServeWidth_BW_A',\n",
    "                   'P1_ServeWidth_C_A',\n",
    "                   'P1_ServeWidth_W_A',\n",
    "                   'P1_ServeDepth_CTL_A',\n",
    "                   'P1_ServeDepth_NCTL_A',\n",
    "                   'P1_ReturnDepth_D_A',\n",
    "                   'P1_ReturnDepth_ND_A',\n",
    "                   'P2_ServeWidth_B_A',\n",
    "                   'P2_ServeWidth_BC_A',\n",
    "                   'P2_ServeWidth_BW_A',\n",
    "                   'P2_ServeWidth_C_A',\n",
    "                   'P2_ServeWidth_W_A',\n",
    "                   'P2_ServeDepth_CTL_A',\n",
    "                   'P2_ServeDepth_NCTL_A',\n",
    "                   'P2_ReturnDepth_D_A',\n",
    "                   'P2_ReturnDepth_ND_A',\n",
    "                   'P1AceA',\n",
    "                   'P2AceA',\n",
    "                   'P1WinnerA',\n",
    "                   'P2WinnerA',\n",
    "                   'P1DoubleFaultA',\n",
    "                   'P2DoubleFaultA',\n",
    "                   'P1UnfErrA',\n",
    "                   'P2UnfErrA',\n",
    "                   'P1NetPointA',\n",
    "                   'P2NetPointA',\n",
    "                   'P1NetPointWonA',\n",
    "                   'P2NetPointWonA',\n",
    "                   'P1BreakPointA',\n",
    "                   'P2BreakPointA',\n",
    "                   'P1BreakPointWonA',\n",
    "                   'P2BreakPointWonA',\n",
    "                   'P1BreakPointMissedA',\n",
    "                   'P2BreakPointMissedA',\n",
    "                   'P1DistanceRunA',\n",
    "                   'P2DistanceRunA',\n",
    "                   'RallyCountA',\n",
    "                   'P1SetsWon',\n",
    "                   'P2SetsWon',\n",
    "                   'Game_timestamp',\n",
    "                   'P1Rank',\n",
    "                   'P2Rank',\n",
    "                   'Total_time',\n",
    "                   'Surface',\n",
    "                    'ServeWidth',\n",
    "                    'ServeDepth',\n",
    "                    'ReturnDepth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,df_val=data_split(df_copy)\n",
    "df=df[(df.ReturnDepth != \"ServeAce\") & (df.ServeDepth != \"DoubleFault\")]\n",
    "df_val=df_val[(df_val.ReturnDepth != \"ServeAce\") & (df_val.ServeDepth != \"DoubleFault\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline Serve 1: {round(df[df.ServeNumber==1].PointWinner.value_counts()[0]/len(df[df.ServeNumber==1]),3)}\")\n",
    "print(f\"Baseline Serve 2: {round(df[df.ServeNumber==2].PointWinner.value_counts()[0]/len(df[df.ServeNumber==2]),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[df.ServeNumber==1]['PointWinner'].values\n",
    "x=pd.get_dummies(df[df.ServeNumber==1][Game_test_columns])\n",
    "labels=x.columns\n",
    "labels=x.columns\n",
    "scaler = StandardScaler()\n",
    "x=scaler.fit_transform(x)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "folds = 10\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "RFCScore = []\n",
    "adaScore = []\n",
    "XGBScore = []\n",
    "LRScore = []\n",
    "FI_rfc = []\n",
    "FI_ada = []\n",
    "FI_XGB = []\n",
    "metrics_models=[]\n",
    "\n",
    "i=0\n",
    "for train_index, test_index in kf.split(x):\n",
    "\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "\n",
    "    \n",
    "    #RF\n",
    "    \n",
    "    RFC = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "    predRFC = RFC.fit(X_train,Y_train)\n",
    "    y_predRFC = predRFC.predict(X_test)\n",
    "    #scoreRFC = accuracy_score(y_predRFC, Y_test)\n",
    "    RFCScore.append(metrics(Y_test,y_predRFC,X_test,RFC))\n",
    "    FI_rfc.append(RFC.feature_importances_)\n",
    "    \n",
    "    #ADA\n",
    "\n",
    "    ada = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=2, random_state=0))\n",
    "    predADA = ada.fit(X_train, Y_train)\n",
    "    y_predADA = predADA.predict(X_test)\n",
    "    scoreADA = accuracy_score(y_predADA, Y_test)\n",
    "    adaScore.append(metrics(Y_test,y_predADA,X_test,ada))\n",
    "    FI_ada.append(ada.feature_importances_)\n",
    "    \n",
    "    \n",
    "    #XG\n",
    "    model_xg = XGBClassifier(objective ='binary:logistic',min_child_weight=5,gamma=0.2,colsample_bytree=0.9,\n",
    "                                     max_depth=6,learning_rate = 0.01,verbosity=0,use_label_encoder=False)\n",
    "    model_xg.fit(X_train, Y_train);\n",
    "    y_pred_xg = model_xg.predict(X_test);\n",
    "    scoreXG = accuracy_score(y_pred_xg, Y_test)\n",
    "    XGBScore.append(metrics(Y_test,y_pred_xg,X_test,model_xg))\n",
    "    FI_XGB.append(model_xg.feature_importances_)\n",
    "    \n",
    "    #LR\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    y_pred_lr=logreg.predict(X_test)\n",
    "    LRScore.append(metrics(Y_test,y_pred_lr,X_test,logreg))\n",
    "    #FI_LR.append(logreg.feature_importances_)\n",
    "    \n",
    "\n",
    "    i+=1\n",
    "    print(f\"Processing fold {i}\")\n",
    "warnings.filterwarnings('default')    \n",
    "#rf_scores.append(np.mean(RFCScore)*100)\n",
    "#xg_scores.append(np.mean(XGBScore)*100)\n",
    "print(\"\\nModel Accuracy\")\n",
    "\n",
    "\n",
    "averages1(\"RFC\",RFCScore)\n",
    "averages1(\"XGBoost\",XGBScore)\n",
    "averages1(\"Logistic Regression\",LRScore)\n",
    "averages1(\"AdaBoost\",adaScore)\n",
    "\n",
    "print(\"\\n Model feature impact > 0.01\")\n",
    "#plot_feature_importance(list(map(mean, zip(*FI_rfc))),labels,\"Random Forrest\")\n",
    "#plot_feature_importance(list(map(mean, zip(*FI_XGB))),labels,\"XG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[(df.ServeNumber==2) | (df.ServeNumber==0)]['PointWinner'].values\n",
    "x=pd.get_dummies(df[(df.ServeNumber==2) | (df.ServeNumber==0)][Game_test_columns])\n",
    "labels=list(x.columns)\n",
    "x=x.values\n",
    "scaler = StandardScaler()\n",
    "x=scaler.fit_transform(x)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "k=0\n",
    "for i in range(len(df)):\n",
    "    if df['PointWinner'].iloc[i] == df['PointServer'].iloc[i]:\n",
    "        k=k+1\n",
    "baseline=(k/len(df))*100\n",
    "\n",
    "\n",
    "folds = 10\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "RFCScore = []\n",
    "adaScore = []\n",
    "XGBScore = []\n",
    "LRScore = []\n",
    "FI_rfc = []\n",
    "FI_ada = []\n",
    "FI_XGB = []\n",
    "metrics_models=[]\n",
    "\n",
    "i=0\n",
    "for train_index, test_index in kf.split(x):\n",
    "\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "\n",
    "    \n",
    "    #RF\n",
    "    \n",
    "    RFC = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "    predRFC = RFC.fit(X_train,Y_train)\n",
    "    y_predRFC = predRFC.predict(X_test)\n",
    "    #scoreRFC = accuracy_score(y_predRFC, Y_test)\n",
    "    RFCScore.append(metrics(Y_test,y_predRFC,X_test,RFC))\n",
    "    FI_rfc.append(RFC.feature_importances_)\n",
    "    \n",
    "    #ADA\n",
    "\n",
    "    ada = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=2, random_state=0))\n",
    "    predADA = ada.fit(X_train, Y_train)\n",
    "    y_predADA = predADA.predict(X_test)\n",
    "    scoreADA = accuracy_score(y_predADA, Y_test)\n",
    "    adaScore.append(metrics(Y_test,y_predADA,X_test,ada))\n",
    "    FI_ada.append(ada.feature_importances_)\n",
    "    \n",
    "    \n",
    "    #XG\n",
    "    model_xg = XGBClassifier(objective ='binary:logistic',min_child_weight=5,gamma=0.2,colsample_bytree=0.9,\n",
    "                                     max_depth=6,learning_rate = 0.01,verbosity=0,use_label_encoder=False)\n",
    "    model_xg.fit(X_train, Y_train);\n",
    "    y_pred_xg = model_xg.predict(X_test);\n",
    "    scoreXG = accuracy_score(y_pred_xg, Y_test)\n",
    "    XGBScore.append(metrics(Y_test,y_pred_xg,X_test,model_xg))\n",
    "    FI_XGB.append(model_xg.feature_importances_)\n",
    "    \n",
    "    #LR\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    y_pred_lr=logreg.predict(X_test)\n",
    "    LRScore.append(metrics(Y_test,y_pred_lr,X_test,logreg))\n",
    "    #FI_LR.append(logreg.feature_importances_)\n",
    "    \n",
    "\n",
    "    i+=1\n",
    "    print(f\"Processing fold {i}\")\n",
    "warnings.filterwarnings('default')    \n",
    "#rf_scores.append(np.mean(RFCScore)*100)\n",
    "#xg_scores.append(np.mean(XGBScore)*100)\n",
    "print(\"\\nModel Accuracy\")\n",
    "\n",
    "\n",
    "print(f\"Baseline 65%\")\n",
    "averages1(\"RFC\",RFCScore)\n",
    "averages1(\"XGBoost\",XGBScore)\n",
    "averages1(\"Logistic Regression\",LRScore)\n",
    "averages1(\"AdaBoost\",adaScore)\n",
    "\n",
    "print(\"\\n Model feature impact > 0.01\")\n",
    "#plot_feature_importance(list(map(mean, zip(*FI_rfc))),labels,\"Random Forrest\")\n",
    "#plot_feature_importance(list(map(mean, zip(*FI_XGB))),labels,\"XG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimere pointwinner serve 1, 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasplit\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.get_dummies(df[df.ServeNumber==1][Game_test_columns]),df[df.ServeNumber==1]['PointWinner'].values, test_size=0.20, random_state=42)\n",
    "\n",
    "#Feature scaling\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Model for tuning\n",
    "classifier=XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "\n",
    "#RandomSearch\n",
    "random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=25,scoring='accuracy',n_jobs=-1,cv=10,verbose=3)\n",
    "start_time = timer(None)\n",
    "random_search.fit(X_train,y_train)\n",
    "timer(start_time) \n",
    "\n",
    "random_search.best_params_\n",
    "#Grid_Search\n",
    "#grid_search=GridSearchCV(classifier,param_grid=params,scoring='f1',n_jobs=-1,cv=10)\n",
    "#start_time = timer(None)\n",
    "#grid_search.fit(X_train,y_train)\n",
    "#timer(start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[df.ServeNumber==1]['PointWinner'].values\n",
    "x=pd.get_dummies(df[df.ServeNumber==1][Game_test_columns])\n",
    "y_val=df_val[df_val.ServeNumber==1]['PointWinner'].values\n",
    "x_val=pd.get_dummies(df_val[df_val.ServeNumber==1][Game_test_columns])\n",
    "labels=x.columns\n",
    "\n",
    "#undersample = RandomUnderSampler(sampling_strategy=0.7)\n",
    "#x, y = undersample.fit_resample(x, y)\n",
    "#print(x.shape,y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "x_val=scaler.transform(x_val);\n",
    "\n",
    "\n",
    "model_xg = XGBClassifier(objective ='binary:logistic',param=random_search.best_params_)\n",
    "\n",
    "model_xg.fit(X_train, y_train);\n",
    "y_pred_xg = model_xg.predict(X_test);\n",
    "scoreXG = accuracy_score(y_pred_xg, y_test)\n",
    "print(f\"Model training score {round(scoreXG,3)*100}\")\n",
    "val_pred=model_xg.predict(x_val)\n",
    "val_scoreXG = accuracy_score(val_pred, y_val)\n",
    "evaluation_time(y_val,val_pred,model_xg,x_val)\n",
    "plot_feature_importance(model_xg.feature_importances_,labels,\"XGBoost Revised First serve\")\n",
    "plt.savefig(\"Pointwinner_S1_V2_Features - XGboost\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search modele 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasplit\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.get_dummies(df[(df.ServeNumber==2) | (df.ServeNumber==0)][Game_test_columns]),df[(df.ServeNumber==2) | (df.ServeNumber==0)]['PointWinner'].values, test_size=0.20, random_state=42)\n",
    "\n",
    "#Feature scaling\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Model for tuning\n",
    "classifier=XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "\n",
    "#RandomSearch\n",
    "random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=25,scoring='accuracy',n_jobs=-1,cv=10,verbose=3)\n",
    "start_time = timer(None)\n",
    "random_search.fit(X_train,y_train)\n",
    "timer(start_time) \n",
    "\n",
    "random_search.best_params_\n",
    "#Grid_Search\n",
    "#grid_search=GridSearchCV(classifier,param_grid=params,scoring='f1',n_jobs=-1,cv=10)\n",
    "#start_time = timer(None)\n",
    "#grid_search.fit(X_train,y_train)\n",
    "#timer(start_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[(df.ServeNumber==2) | (df.ServeNumber==0)]['PointWinner'].values\n",
    "x=pd.get_dummies(df[(df.ServeNumber==2) | (df.ServeNumber==0)][Game_test_columns])\n",
    "y_val=df_val[(df_val.ServeNumber==2) | (df_val.ServeNumber==0)]['PointWinner'].values\n",
    "x_val=pd.get_dummies(df_val[(df_val.ServeNumber==2) | (df_val.ServeNumber==0)][Game_test_columns])\n",
    "labels=x.columns\n",
    "\n",
    "#undersample = RandomUnderSampler(sampling_strategy=0.9)\n",
    "#x, y = undersample.fit_resample(x, y)\n",
    "#print(x.shape,y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "x_val=scaler.transform(x_val);\n",
    "model_xg = XGBClassifier(objective ='binary:logistic',params=random_search.best_params_)\n",
    "\n",
    "\n",
    "model_xg.fit(X_train, y_train);\n",
    "y_pred_xg = model_xg.predict(X_test);\n",
    "scoreXG = accuracy_score(y_pred_xg, y_test)\n",
    "print(f\"Model training score {round(scoreXG,3)*100}\")\n",
    "\n",
    "val_pred=model_xg.predict(x_val)\n",
    "val_scoreXG = accuracy_score(val_pred, y_val)\n",
    "evaluation_time(y_val,val_pred,model_xg,x_val)\n",
    "plot_feature_importance(model_xg.feature_importances_,labels,\"XGBooost Revised Second Serve \")\n",
    "plt.savefig(\"Pointwinner_S2_V2_Features - XGboost\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=[500,800,1500,2500,5000]\n",
    "max_features=['auto','accuracy','log2']\n",
    "max_depth=[10,20,30,40,50]\n",
    "min_samples_split=[2,5,10,15,20]\n",
    "min_samples_leaf=[1,2,5,10,15]\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[Game_test_columns],df['PointWinner'].values, test_size=0.20, random_state=42)\n",
    "\n",
    "#Feature scaling\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) \n",
    "\n",
    "#define classifier\n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "#Define search\n",
    "random_search=RandomizedSearchCV(estimator=RFC,param_distributions=random_grid,n_iter=10,n_jobs=-1,cv=3,verbose=3)\n",
    "\n",
    "#Run search\n",
    "start_time = timer(None)\n",
    "#random_search.fit(X_train,y_train)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable\n",
    "\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param={'n_estimators': 1500,\n",
    " 'min_samples_split': 5,\n",
    " 'min_samples_leaf': 15,\n",
    " 'max_features': 'accuracy',\n",
    " 'max_depth': 10,\n",
    " 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y=df['PointWinner'].values\n",
    "x=df[Game_test_columns].values\n",
    "y_val=df_val['PointWinner'].values\n",
    "x_val=df_val[Game_test_columns].values\n",
    "labels=Game_test_columns\n",
    "\n",
    "#x,y=sample(x,y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "x_val=scaler.transform(x_val);\n",
    "\n",
    "\n",
    "RFC = RandomForestClassifier(n_estimators=1500,min_samples_split=5,min_samples_leaf=15,\n",
    "                            max_features=\"log2\",max_depth=10,bootstrap=True)\n",
    "predRFC = RFC.fit(X_train,y_train)\n",
    "val_pred = predRFC.predict(x_val)\n",
    "scoreRFC = accuracy_score(val_pred, y_val)\n",
    "evaluation_time(y_val,val_pred,RFC,x_val)\n",
    "plot_feature_importance(RFC.feature_importances_,labels,\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Kandidat_2_semester')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7a3814ace472c9a6020303c0b948e4dc3352b5fa23366fb0cb9a993484e2f15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
